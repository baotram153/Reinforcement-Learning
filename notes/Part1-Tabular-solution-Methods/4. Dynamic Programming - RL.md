- What is Dynamic Programming?
	- a collection of algorithms that computes the optimal policy after being given a perfect model of the environment as a Markov Decision Process
	- DP algorithms are obtained by turning Bellman equations into assignments, that is into update rules to improve the approximation of the value functions
## 1. Policy Evaluation
- What is Policy Evaluation?
	- The process of computing the state-value $v_\pi$ for an arbitray policy $\pi$
- When does the existence and uniqueness of $v_\pi$ is guarantee?
- Bellman equation for state-value function: [[Markov Decision Processes#^Bellman-equation]]
- If the environment's dynamics is known, the above equation will give us?
	- a system of $|\mathcal S|$ linear equations in $\mathcal S$ unknowns
	- each unknows is $v_\pi(s) \in \mathcal S$
- We can solve for $v_\pi(s)$ for each $s \in \mathcal S$ using that system, however it's computational expensive (even impossible if we have a large $\mathcal S$) $\rightarrow$ iterative method
- Iterative policy evaluation
	- intuition
		- we have a sequence of value function $v_0, v_1, v_2,...$, each maps $v_\pi(s)$ for each $s \in \mathcal S$ into $R$ (a function can also be described as discrete mappings, not just by parameters)
		- value function $v_0$ is the mapping of each state to and arbitrary value (except for terminal states - value 0)
		- value function for each successive approximation is updated by using Bellman equation for $v_\pi$
	- $$v_{k+1}(s) = \sum_{a \in A}\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r + \gamma v_k(s')\right]$$
		- $(k+1)^{th}$ approximation
		- the sequence can be shown to converge to $v_\pi$ as $k \rightarrow \infty$
		- can be thought of as a replacement of old value of $s$ with a new value obtained by
			- the value of each successive state (along all possible one-step transitions under policy $\pi$)
			- the immediate reward
	- each update is called an *expected update* since they are based on an expectation over all possible next states rather than on a sample next state
- Ways to update (we think of each update as a sweep through the state space)
	- use 2 arrays
	- use 1 array and update the values "in place"
		- sometimes new values are used instead of old ones on the right hand side of the equation
		- usually converges faster $\rightarrow$ used more in DP algorithms
- ![[Pasted image 20230920094914.png]]
## 2. Policy Improvement
- Why?
	- Our reason for computing the value function for a policy is to help find better policies
- Idea
	- How to know if new policy is better than old policy
		- select $a$ and thereafter following the existing policy $\pi$, we have the value of state $s$ computed by $$q_\pi(s,a) \doteq \sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right]$$
		- if $q_\pi(s,\pi'(s)) \geq v_\pi(s)$, then policy $\pi'$ is better than $\pi$
		- Select at each state the action that appears best according to $q_\pi(s,a)$ $$\begin{align} \pi'(s) & = \arg \max_a q_\pi(s,a) \\ & = \arg \max_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')] \end{align} \qquad (1)$$
		- When $v_\pi'(s) = v_\pi(s)$, then $\forall s \in \mathcal S$, we have (according to $(1)$) $$\begin{align} v_\pi'(s)  & = \max_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')] \\ & = \max_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_{\pi'}(s')] \\ \end{align}$$
			- This is Bellman optimality equation $\rightarrow$ $\pi'$ must be $\pi_*$ and both $\pi$ and $\pi'$ are optimal policy
## 3. Policy Iteration
- ![[Pasted image 20230920105743.png]]
	- because finite MDP has finite number of deterministic policies, thiss process must converge to an optimal policy
- ![[Pasted image 20230920110709.png]]
## 4. Value Iteration
- If policy evaluation is done iteratively, then convergence exactly to $v_\pi$ occurs only in the limit. Must we wait for exact convergence, or can we stop short of that?
- Value iteration intuition
	- the update operation combines the policy improvement and truncated policy evaluation
	- $$v_{k+1}(s) = \max_a\sum_{s',r}p(s',r|s,a)[r+v_k(s')]$$
- ![[Pasted image 20230920152052.png]]
	- each sweep of value iteration includes one sweep of policy valuation and one sweep of policy improvement
	- a whole class of truncated policy evaluation can be thought of as sequences of sweeps, some of which use policy evaluation and some of which use value iteration updates
## 5. Asynchronous Dynamic Programming
- for problems with a large state set, a sweep through whole state set in each iteration is impossible
- in assynchronous dynamic programming, the values of some states are updated several times before the values of others are updated once
- one version updates the value of only one state, $s_k$, on each step $k$, using the value iteration update
- asynchronous truncated policy iteration: intermix policy evaluation and value iteration updates
- intermix computation with real-time interaction $\rightarrow$ focus the DP algorithm's updates onto states that are most relevant to the agent
## 6. Generalized Policy Iteration
## 7. Effectiveness of dynamic programming