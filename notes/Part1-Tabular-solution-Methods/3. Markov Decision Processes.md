- MDPs: states are associative, action in each state affects sequential states and future reward

### 3.1 The Agent-Environment Interface
- agent, environment
- ![[Pasted image 20230916063000.png]]
- **finite** MDP: the sets of states, actions, and rewards (S, A, and R) all have a finite number of elements
- for given preceding state and action, $R_t$, $S_t$ have discrete propability distribution
	- $$p(s', r|s,a) \doteq Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}$$
	- $p$ defines the dynammics of the MDP
	- notice that
		- $$\sum_{s' \in S}\sum_{r \in R} p(s',r|s,a) = 1, \forall s \in S, a \in A(s)$$
- in a Markov decision process, the probabilities given by $p$ completely characterize the environment’s dynamics, nghĩa là mỗi state $s'$ và reward $r$ chỉ bị ảnh hưởng bởi state $s$ và action $a$ trước đó
	- restriction not on the decision process, but on the state $\rightarrow$ this state has Markov property
- one can also compute
	- state-transition probabilities $p:S \times S \times A \rightarrow [0,1]$ $$p(s'|s,a) = Pr(S_t=s'|S_{t-1} = s, A_{t-1} = a) = \sum_{r \in R}p(s', r| s,a)$$
	- Expected reward for state-action pair $r:S \times A \rightarrow R$ $$r(s,a) = E[R_t | S_{t-1} = s, A_{t-1} =a] = \sum_{r \in R}r \sum_{s' \in S} p(s',r|s,a)$$
	- Expected reward for state-action-next state triples $r: S \times A \times S \rightarrow R$ $$r(s',a,s) = E[R_t | S_{t-1}=s, A_{t-1} = a, S_t = s'] = \frac{\sum_{r \in R}r \cdot p(s',r|s,a)}{p(s'|s,a)}$$
- Example: Recycling robot
	- ![[Pasted image 20230916071301.png]]
### 3.2  Goals and Rewards
- the agent always learns to maximize its reward. if we want it to do something for us, we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals

### 3.3 Returns and Episodes
- in simpliest case the return is the sum of rewards $$G_t = R_{t+1} + R_{t+2} + R_{t+3} + ··· + R_T$$
	- where $T$ is the final time step
- *episodic task*: tasks where the agent-environment interaction breaks naturally into subsequences
	- episodes, terminal state
	- the next episode begins independently of how the previous one ended
	- the set of all nonterminal states, denoted $S$, the set of all states plus the terminal state, denoted $S^+$
	- $T$: a random variable that normally varies from episode to episode
- *continuing tasks*
	- $T = \infty \rightarrow$ return could easily be $\infty$ 
- another approach: *discounting*
	- the agent tries to maximize the sum of the *discounted rewards* it receives over the future $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$$
		- where $0 \leq \gamma \leq 1$ called the discounted rate
- relation between returns of successive time steps $$\begin{align} G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4}...\\ 
 & = R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} ...) \\
 & = R_{t+1} + \gamma G_{t+1}\end{align}$$
 $$G_t = R_{t+1} + \gamma G_{t+1}$$
### 3.4  Unified Notation for Episodic and Continuing Tasks
- $S_{t,i}$: the state representation at time $t$ of episode $i$ -> never have to distinguish between different episodes, always consider a particular episode, or state something that is true for all episodes -> write $S_{t}$ to refer to $S_{t,i}$
- consider episode termination to be the entering of a special *absorbing state* that transits only to itself and that generates only rewards of zero
	- ![[Pasted image 20230916074123.png]]

### 3.5   Policies and Value functions
- value functions - functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state
	- the notion of “how good” here is defined in terms of future rewards that can be expected (expected return)
- policy - a mapping from states to probabilities of selecting each possible action
	- $\pi(a|s)$ is the probability that $A_t = a$ if $S_t = s$
- state value function: expected return when starting in s and following $\pi$ thereafter $$v_\pi(s) = E[G_t | S_t = s], \forall s \in S$$
- state-action value function: expected return starting from $s$1, taking the action $a$, and there after following policy $\pi$ $$q_\pi(s,a) = E[G_t| S_t = s, A_t = a], \forall s \in S, a\in A$$
- Bellman equation - recursive relationship of value function ^Bellman-equation
	- $$\begin{align} v_\pi(s) & = \mathbb E[G_t | S_t = s] \\
	& = \mathbb E[R_{t+1} + \gamma{G_{t+1}} | S_t = s] \\
	& = \sum_{a \in A}\pi(a|s)\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)[r + \gamma \mathbb E[G_{t+1} | S_{t+1} = s'] \\
	& = \sum_{a \in A}\pi(a|s)\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)[r + \gamma v_\pi(s')], \qquad \forall s \in S
	\end{align}$$
	- $$\begin{align} q_\pi(s,a) & = \sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)\left(r + \gamma v_\pi(s')\right) \\
	& =\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)\left(r + \gamma \mathbb E[G_{t+1}|S_{t+1}=s']\right) \\
	& = \sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)\left(r + \gamma \sum_{a' \in A}\pi(a'|s') \mathbb E[G_{t+1}|S_{t+1} = s', A_{t+1} = a']\right) \\
	& = \sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)\left(r + \gamma \sum_{a' \in A}\pi(a'|s')q_\pi(s',a')\right) \\
	\end{align}$$
- Backup diagram 
	- ![[Pasted image 20230916080735.png]]
	- diagram backup operations - transfer value information back to a state (or a state–action pair) from its successor states (or state–action pairs)
### 3.6  Optimal policies and Optimal value functions
- $\pi \geq \pi'$ if and only if $v_\pi(s) \geq v_{\pi'}(s)$ for all $s \in S$. there is always at least one policy that is better than or equal to all other policies -> optimal policy
	- $$v_*(s) = \max_\pi v_\pi(s) \quad \forall s \in S$$
	- $$q_*(s,a) = \max_\pi q_\pi(s,a) \quad \forall \text{ state - action pairs } s, a \in S, A$$
- the Bellman optimality equation - the value of a state under an optimal policy must equal the expected return for the best action from that state
	- $$\begin{align}
	v_*(s) & = \max_{a \in A} q_{\pi_*}(s,a) \\
	& = \max_a \mathbb E_{\pi_*}[R_{t+1} + \gamma G_{t+1}|S_t = s, A_t = a] \\
	& = \max_a\sum_{s',r}p(s',r|s,a)(r + \gamma E_{\pi_*}[G_{t+1}| S_{t+1} = s']) \\
	& = \max_a\sum_{s',r}p(s',r|s,a)(r + \gamma v_*(s')) \\
	\end{align}$$
	- $$\begin{align} q_*(s,a) & = \mathbb E[R_{t+1} + \gamma v_*(S_{t+1})| S_t=s, A_t = a] \\
	& = \sum_{s'} \sum_{r} p(s',r|s,a)(r + \gamma \max_{a'}q_*(s,a')) 
	\end{align}$$
	- ![[Pasted image 20230916083025.png]]
- solution to the Bellman equation is rarely directly useful because it's the same to exhaustive search. this solution relies on at least 3 assumptions
	- (1) the dynamics of the environment are accurately known
	- (2) computational resources are sufficient to complete the calculation
	- (3) the states have the Markov property

### Intuition
- equation of $v_\pi$ according to $q_\pi$ and $\pi$ $$\begin{align} 
v_\pi(s) & = \mathbb E[G_t | S_t = s] \\
& = \sum_{a \in A}\pi(a|s) \mathbb E[G_t|S_t = s, A_t = a] \\
& = \sum_{a \in A}\pi(a|s) q_\pi(s,a)
\end{align}$$
- equation of $q_\pi$ according to $v_\pi$ $$\begin{align}
q_\pi(s,a) & = \mathbb E[G_t | S_t = s, A_t = a] \\
& = \sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)[r + \gamma v_\pi(s')]
\end{align}$$
- $$\begin{align} V(s)& = \sum_{a \in A}\pi(a|s)\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)[r + \gamma v_\pi(s')] \\
& = \frac{1}{3}\times1\times(-1+0) + \frac{1}{3}\times1\times(-1-1)+\frac{1}{3}\times 1 \times (-1-1) \\
& = -\frac{5}{3} \approx -1.7
  \end{align}$$
