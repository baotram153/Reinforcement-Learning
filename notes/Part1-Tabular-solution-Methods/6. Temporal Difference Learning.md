
### 1. TD prediction
- a MC update: $$V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)]$$
	- $G_t$ cannot be calculated till we get to the end of the episode
- a TD update: $$V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$$
	- only wait a step ahead to estimate value of current state $\rightarrow$ one-step TD or TD(0)
	- the term in the square bracket can be consider as the error between the current estimated value of $S_t$ and a better estimation of it $R_{t+1} + \gamma V(S_{t+1})$
		- if we consider $V(s)$ does not change for the course of the episode, then we have the MC error equals the discounted sum of the TD error $$\begin{align} 
		G_t - V(S_t) & = R_{t+1} + \gamma G_{t+1} + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) - V(S_t) \\
		& = [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] + \gamma[G_{t+1} - V(S_{t+1})] \\
		& = \delta_t + \gamma(\delta_{t+1} + \gamma[G_{t+2} - V(S_{t+2})]) \\
		& = \delta_t + \gamma \delta_{t+1} + \gamma^2\delta_{t+2} + ... + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-1}[G_T - V(S_T)] \\
		& = \sum_{k=t}^{T-1} \gamma^{k-t}\delta_t  \end{align}$$
		if $V(s)$ is updated in every step? (read later)
	- called a *sample update* because its updates only base on certain sample sucessor instead of the whole distribution of all possible sucessors $\neq$ *expected update* 
		- ![[Pasted image 20230924111257.png]]
	- each update bases on an existing estimate ($\approx$ DP ) $\rightarrow$ bootstrapping methods 
- intuition $$\begin{align} v_\pi(s) & = \mathbb E[G_t | S_t = s] \qquad (1) \\ & = \mathbb E[R_{t+1} + \gamma G_{t+1}|S_t = s] \\
 & = \mathbb E[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \qquad (2)\end{align}$$
	 - MC uses the estimate of $(1)$ as the target, while TD learning uses the estimate of $(2)$ as a target
		 - target of MC is an estimate because it only bases on a single sample, not a whole distribution of states and reward for every state-action pair (not *expected return*)
		 - target of DP is an estimate but not for the same reason, but because the $v_\pi(S_{t+1})$ is just a current estimate of the value - $V(S_{t+1})$
		 - target off TD learning is an estimate for both reasons above
	
- algorithm ![[Pasted image 20230924104216.png]]
- example: driving home
	- reward: of each state is the elapsed time when go from previous state to current state (if our goal is to minimize this elapsed time, then reward would be negative)
	- return: time estimated to go from that state to home (because return is the cummulative of reward and here we are not discounting : $\gamma =1$)
	- ![[Pasted image 20230924163446.png]]
		- elapsed time from each state = elapsed time from previous state - elapsed time from current state = return
		- predicted time to go = estimate of the value of each state
		- predicted total time: show the relationship between elapsed time and predicted time to go for each state
	- changes recommended by MC (left) and TD (right)![[Pasted image 20230924171131.png]]
		- $S_{t}$ = state when exiting highway and $V_k(s)$ is the value of state $s$ at update $k^{th}$
		- MC methods: 
			- $$\begin{align} \text{predicted time to go} & = V_{k+1}(S_t) = V_k(S_t) + \alpha(G_t - V_k(S_t)) \\
			 & = 15 + 1\times(23 -15) \\
			 & = 23\end{align}$$
			 - $\text{predicted total time = elapsed time + predicted total time} = 20 + 23 = 43$
		 - TD methods
			 - $$\begin{align} \text{predicted time to go} & = V_{k+1}(S_t) = V_k(S_t) + \alpha(R_{t+1} + \gamma V_k(S_{t+1}) - V_k(S_t)) \\
			 & = 15 + 1\times((30 - 20) + 10 - 15) \\
			 & = 20 \end{align}$$
			 - $\text{predicted total time = elapsed time + predicted total time} = 20 + 20 = 40$
	- predicted time to go (estimated value of each state) vs predicted total time
		- ![[Pasted image 20230924171106.png]]
### 2. Advantages of TD Prediction methods
- do not need a model of the environment
- online, fully incremental fashion
- can be applied to applications that have very long episodes, or continuing tasks
- learn from each transition regardless of what subsequent actions are taken $\rightarrow$ avoid problem of MC, in which we havee to ignor or discount episodes on which experimental actions are taken
- *in practice*, TD methods converges faster than constant-$\alpha$ MC
- example: random walk
	- MRP: MDP without actions (in each state we have a single probability distribution of its sucessing states and rewards)
	- ![[Pasted image 20230924180844.png]]
	- because the reward is not discounted, the true value of each state is the probability of reaching right terminal state from that state $\rightarrow$ the true values of all states, from A to E is $\frac{1}{6}, \frac{2}{6}, ... \frac{5}{6}$, respectively
	- ![[Pasted image 20230924181157.png]]
### 3. Optimality of TD(0)
- batch updating:
	- present the experience repeatedly until the values are converged upon an answer
	- the increment is computed in each time step, while the value function is changed only (after the whole experience, say 10 or 100 episodes)
	- ![[Pasted image 20230924182530.png]]
- why TD perform better? (read later)
	- Monte Carlo method is optimal only in a limited way, and that TD is optimal in a way that is more relevant to predicting returns (?)
- example: you are the predictor
	- ![[Pasted image 20230924182607.png]]
	- what's the value of state $B$? $0.75$
	- what's the value of state $A$? $0$ (MC) or $0.75$ (TD)
		- ![[Pasted image 20230924182829.png]]
	- we expect that the TD estimate produce lower error on future data, even though the MC estimate is better on existing data
### 4. Sarsa : On-policy TD Control
- instead of consider transitions from state to state, we consider transitions from state-action pairs to state-action pairs
	- ![[Pasted image 20230924235322.png]]
- the rule use every elements of the quintuple of events $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ $\rightarrow$ Sarsa
- the policy converge to the limit of the greedy policy (which can be arranged, for example by setting $\epsilon = \frac{1}{t}$)
- ![[Pasted image 20230925000103.png]]
### 5. Q-learning: Off-policy TD Control
- breakthrough in reinforcement learning (why?)
- ![[Pasted image 20230925000741.png]]
- ![[Pasted image 20230925001005.png]]
### 6. Expected Sarsa
- ![[Pasted image 20230925001103.png]]
- more complex computationally than Sarsa, but eliminate the variance due to random selection of $A_t$
- (?) insert performance vs Sarsa and QL
- can be used on-policy and off-policy, in which $\pi$ is greedy and $b$ is exploratory $\rightarrow$ generalize Q-learning
### 7. Maximization Bias and Double Learning
- what is maximization bias
	- all value update rules above have the maximization operations
	- in these algorithms, the maximum of estimated values is used implicitly as an estimate of the maximum (of true) value $\rightarrow$ maximization bias (consider a state with $q(s,a)$ for all $a$ is zero but the estimated value fluctuate around $0$, the maximum of the true values is zero while the maximum of the estimated values is positive $\rightarrow$ positive bias)
- example
	- Q-learning
	- Double Q-learning
- double Q-learning algorithm
	- idea
		- Have 2 different sets of values $Q_1$ and $Q_2$ and try to estimate the values in each set independently
		- Use 1 estimate to determine the maximizing action $A^*=\arg \max_a Q_1(a)$
		- Use other estimate to estimate the action value $Q_2(A^*) = Q_2(\arg \max_a Q_1(a))$
		- Repeat the process with the role of 2 estimates reversed
	- algorithm
		- ![[Pasted image 20230925004410.png]]
			- how each estimate is updated
			- to be objective, the behavior policy use the average of the two action-value estimates
### 8. Games, Afterstates, and other special cases
- example from tic-tac-toe
		- conventional state-value function
	- state-value function in tic-tac-toe $\rightarrow$ afterstate
- afterstate: know the initial part of the environement's dynamics but not the full dynamics
	- chess: you know the immediate state after making a move, not how our opponent will reply
- why it is effective to implement tic-tac-toe algorithm in terms of afterstate-value function?
	- there are state-move pairs where the after states are the same, thus they should have the ***same action value***
		- ![[Pasted image 20230925010044.png]]
	- in conventional action-value function, those action value are estimated separately, while an afterstate-value function would assess both equally (any learning on the position - move pair on the left should be transfer to the one on the right)