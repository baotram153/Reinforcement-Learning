Note: in some places "reward" can be understood as "return"
### 1. A k-armed Bandit Problem
- definition
	- faced repeatedly with a choice among k different options, or actions
	- reward chosen from a ==stationary== probability distribution
	- this is the simple version of the Bandit problem
- solution
	- each of the k actions has an expected or mean reward given that that action is selected -> value
	- $$q_*(a) = E[R_t|A_t = a]$$
	- always select the action with highest value
	- the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$
	- at every timestep there're always actions with highest estimated value
		- greedy actions -> exploiting
		- non-greedy actions -> exploring
		- whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps
### 2. Action-value method
- estimating values of actions
	- ![[Pasted image 20230902084340.png]]
	- as the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$
	- sample-average method
- methods of action selection
	- greedy action selection
	- $\epsilon$ - greedy action selection
		- as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to their respective $q_*(a)$
		- the probability of selecting the optimal action converges to greater than 1 - $\epsilon$
			- just asymptotic guarantees, practically doesn't very helpful
### 3. The 10-armed Testbed
1. Definition of The 10-armed Testbed
	- a set of 2000 randomly generated k-armed bandit problems with k = 10
	- the true reward $q_*(a)$ of each of the ten actions was selected according to a normal distribution with mean zero and unit variance
	- the actual rewards were selected according to a mean $q_*(a)$, unit-variance normal distribution
		- ![[Pasted image 20230902084945.png]]
	- 1000 time steps, repeat 2000 runs -> average out
2. Result
	- the increase in expected reward with experience ![[Pasted image 20230902085145.png]]
- ![[Pasted image 20230902085155.png]]
3. Comparison between the greedy and the $\epsilon$ - greedy method 
	- the amount of noise
	- stationarity
### 4. Incremental implementation
- $$\begin{align}
Q_{n+1} & = \frac{1}{n} \sum_{i=1}^{n}R_i \\
& = \frac{1}{n} \left( R_n + \sum_{i=1}^{n-1}R_i \right) \\
& = \frac{1}{n} \left[ R_n + (n-1)Q_n \right] \\
& = Q_n + \frac{1}{n}(R_n - Q_n)
\end{align}$$
- Generally $$\text{NewEstimate} = \text{OldEstimate} + \text{StepSize}(\text{Target} - \text{OldEstimate})$$
### 5. Tracking nonstationary problem
- nonstationary -> the probability distribution of reward changes over time -> make sense to give more weight to recent rewards than to long-past rewards
- $$\begin{align} 
Q_{n+1} & = Q_n + \alpha(R_n - Q_n) \\
& = \alpha R_n + (1-\alpha)Q_n \\
& = \alpha R_n + (1-\alpha)(\alpha R_{n-1} + (1 - \alpha)Q_{n-1}) \\
& = \alpha R_n + \alpha (1-\alpha)R_{n-1} + (1-\alpha)^2(\alpha R_{n-2} + (1 - \alpha)Q_{n-2}) \\
& = \alpha R_n + \alpha(1-\alpha)R_{n-1} + ...+ \alpha(1-\alpha)^{n-1}R_1 + (1-\alpha)^nQ_1 \\
& = (1-\alpha)^nQ_1 +\alpha \sum_{i=1}^n(1-\alpha)^{n-i}R_i
\end{align}$$
	- where $Q_1$ is the initial average reward (set by programmer)
	- note
		- weighted average method
		- the sum of the weights is $(1-\alpha)^n + \alpha \sum_{i=1}^n (1-\alpha)^{n-i} = 1$ (?)
		- intuition: when $i$ increases -> $n-i$ decreases -> $(1-\alpha)^{n-i}$ increases -> recent reward takes on more weight than long-passed reward
- convergence conditions 
	- $$\sum_{i=1}^\infty\alpha_n(a) = \infty \quad (1) \qquad \qquad \qquad \sum_{i=1}^\infty \alpha^2_n(a) < \infty \quad (2)$$
	- intuition
		- $(1)$ assures that the step size is big enought to overcome any initial conditions or random fluctuations
		- $(2)$ assures that eventually the step size will be small enough for the average reward to converge
	- note
		- sample-average case: $\text{StepSize} = \frac{1}{n}$ satisfies convergence conditions
		- weighted average case: $\text{StepSize} = \alpha$ satisfies the first condition but doesn't satisfies the second $\rightarrow$ average reward doesn't converge, which is desirable since our problem is non-stationary
### 6. Optimistic initial values
- $Q_1$ has to be chosen by us, instead of randomly initialize it or set it to 0, we could use it to provide some prior knowledge to our agent
- biased by initial condition
	- sample-average case: $Q_1$ disappears when every actions are done once ($Q_2 = Q_1 + \frac{1}{1}(R_1 - Q_1) = R_1$)
	- weighted average case: average reward is always affected by $Q_1$, though that effect reduces over time ($(1-\alpha)^nQ_1$)
- intuition
	- when setting $Q_1$ to a high value, when choosing an action and receive a reward, the agent will be "disappointed" by that reward (though that action might have the highest true reward, others actions will have higher average reward due to optimistic initialization)
	- $\rightarrow$ a greedy algorithm will try other actions, and after that continue trying each action multiple times
	- $\rightarrow$ to a certain time when the average reward of each action approach their true reward, the exploration stops, the algorithm becomes really greedy
- result
	- ![[Pasted image 20230913121141.png]]
	- first few steps is exploration phase, % optimal action increases really slow
- some drawbacks
	- does not generalize good, especially for non-stationary problems where exploration is need throughout the agent's life time, not just at the beginning time steps
### 7. Upper-Confidence-Bound action selection
- Idea: it's better to choose an action according to its **potential** to being optimal
	- how close the estimates are to being maximal
	- the uncertainties of those estimates
- Formula $$A_t = \arg \max_a \left( Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}} \right)$$
	- when $N_t(a) = 0$, that action is a maximizing action (action that has to be made in that time step)
- Intuition
	- $Q_t(a)$ is the estimate of the average reward of that action, $Q_t(a)$ high means that action is close to optimal and is desirable
	- $c$ is the confidence level ($c>0$), control the degree of exploration
	- the square root term is the uncertainty / variance
	- after every time step, the $\ln t$ increases, if an action is not chosen, its $N_t(a)$ term doesn't increase $\rightarrow$ uncertainty increases, causes the action to have a higher possibility to be chosen in next time steps
	- the term to be maximized can be thought of as an upper bound of the average reward estimated for an action
	- the use of $\ln t$ indicates that the increment in uncertainty will reduce over time, but it isn't bounded
- result
	- often performs well
	- couldn't be generalized well, especially for problems that
		- are non-stationary (because of $\ln t$ ?)
		- have a large state space
	- ![[Pasted image 20230913164047.png]]
### 8. Gradient Bandit Problem
- Choose an action not base on its estimated average reward but base on its numerical preference, denoted by $H_t(a) \in R$
	- the probability of an action being chosen is defined as $$\pi(a) = \frac{e^{-H_t(a)}}{\sum_{\mathcal b \in S_a} e^{-H_t(b)}}$$
		- where $\mathcal S_a$ is the set of all actions
- Update preference
	- Update preference of the action being taken at time step $t$ $$H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))$$
	- Update preference of other actions at time step $t$ $$H_{t+1}(a) = H_{t+1}(a) - \alpha(R_t - \bar{R}_t)\pi_t(a)$$
	- intuition
		- if the reward of the action being taken $A_t$ increases, the preference for that action increases since $(R_t - \bar{R}_t)$ is positive
		- the rate of increment / decrement also depend on the probability of taken that action in the policy $\pi_t(A_t)$, the amound of increment / decrement is larger if the probability of that action is small, and vice versa
		- the similar intuition applies to cases of non-taken actions
- Understand the Gradient Bandit Problem as a stochastic estimation of gradient ascent (xem sau)


