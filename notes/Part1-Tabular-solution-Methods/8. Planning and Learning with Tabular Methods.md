### 1. Planning and Learning
- model-based learning (learning from experience generated by a model) and model-free learning (learning from real experience)
- model-based learning
	- distribution model: gives complete state-reward distribution of all state-action pairs
	- sample model: produce one sample at a time from the distribution
	- conclusion
		- distribution model is more powerful but hard to be obtained (use sample model a lot of times to obtain distribution model)
- state-space learning and plan-space learning
	- state-space learning: simulated experience -> update value of each state -> improve policy 
	- plan-space learning: search through space of plans to find the best plan, operations are aimed to evolve current plans to better plans (evolutionary methods)
- the only different between planning and learning is the source of experience -> methods can be transferred between learning and planning
	- model-based -> planning, model-free -> learning?
### 2. Dyna-Q
- an architecture integrating major functions needed for an online learning agent
	- ![[Pasted image 20231027113115.png]]
- real experience serves 2 purposes
	- update value function / policy -> direct reinforcement learning
	- model-learning -> planning to update value function / policy -> indirect reinforcement learning
- advantages and disadvantages of direct and indirect RL 
	- indirect: good for situations where interaction with the environment is limited 
	- direct: describe correctly the environment dynamic, prevent bias from model constructing
- in dyna-Q, the algorithm used for planning is random-sample one-step Q-learning, for direct learning is one-step Q-learning
- ![[Pasted image 20231027115438.png]]
- note: rõ hơn về model-base: mục tiêu của model là lưu lại dynamic của môi trường, tức là xác suất đạt được một state nhất định khi thực hiện 1 state-action nhất định, còn reward hoặc value tính toán được thì không phải là dynamic của môi trường, được sử dụng cho cả direct và indirect learning
### 3. The Model is wrong
- Reasons
	- The environment is stochastic and a small number of samples are observed
	- The model is a function approximation that is failed to generalize
	- The environment changes over time
- Mis-representation of the environment -> suboptimal policy -> discovery and correcting modeling error
- Example
	- Blocking Maze ![[Pasted image 20231010212936.png]]
	- What if the environment changes for the better?
		- Shortcut Maze ![[Pasted image 20231010213510.png]]
			- even with $\epsilon-$greedy policy, the number of exploration steps cannot be large enough for the agent to discover the new shortcut
			- idea of Dyna-Q+
				- bonus reward for each state-action pair $$r' = r+k\sqrt{\tau}$$
					- where $\tau$ is the number of time steps that that $s-a$ pair hasn't been tried
### 4. Prioritize Sweeping
- Idea
	- It's better to prioritize updates that
		- change the value of a state-action pair (it makes no sense update the value of states whose values of all successor states are zero)
		- make a bigger change in value (by updating the value of some state-action pairs, predictions can be made about how much other state-action values will be changed according to that. we should use those prediction to prioritize updates)
- Backward focusing: prioritize updating states that leads to states whose values are changed in previous updates
- ![[Pasted image 20231010220321.png]]
- Prioritize sweeping use the expected update of each state-action pair instead of sample update (why? -> xem lại)-> limitation: sometimes perform worse than sample update
	- Sample update can win because they break the overall computation of expected update into update of a single transition, but based on (scaled with) the probability of that transition without sampling ("small backup")
- Forward focusing: prioritize updating states that can be easily reached from the states that are frequently reached according to current policy
### 5. Expected vs. Sample Updates
- 3 binary dimension of how to compute value function
	- state value or state-action value $\quad(1)$
	- value function for optimal policy or abitrarily given policy $\quad (2)$
	- expected updates or sample updates $\quad (3)$
		- 2 first dimensions gives us $q_*, v_*, q_\pi, v_\pi$
- Backup diagrams for all the one-step updates ![[Pasted image 20231010225623.png]]
- sample update's correctness depends on correctness of the value of it's successor state-action pair + sampling error, but is cheaper computationally
	- suppose $b$ is the branching factor (number of possible next state give a state-action pair) $\rightarrow$ computation required is $b$ times greater using expected update than using sample update
	- if computation is limited, it's better to compute sample updates of many pairs instead of expected updates of a few pairs
- comparison of expected and sample update ![[Pasted image 20231010232020.png]]
	- The values at the next states are assumed correct, so the expected update reduces the error to zero upon its completion
	- large value of $b$: sample update reduces error in value estimate dramatically in a small number
	- in pratice, by sooner better the estimate of successor states, estimated values of previous states are backened up more accurately
### 6. Trajectory Sampling
- first (classical) approach: exhaustive sweep is problematic for problems with large state space
- second approach: update state / state-action pairs according to samples generated from some distribution
	- uniform distribution -> same problem as exhaustive sweeps
	- on-policy distribution: distribution observed when following current policy
		- one generate an abitrary first state, choose an action sampled from our current policy, received state, reward sampled from model's distribution
		- in other word, one simulate its own trajectory using current policy, and update the value of states / state-action pairs that it encounter
- focus on on-policy distribution is beneficial in that it ignores states that are not really relevant, but it could be detrimental when the same set of states are evaluated over and over
- comparison of agents doing value updates based on on-policy distribution and uniform distribution. At certain state of evaluation process, the value of start state is evaluated as if the agent follows the greedy policy
	- ![[Pasted image 20231027173120.png]]
	- on-policy do better in the start but poorer in the long run. Since at the start, value of states that are near the start are evaluate again and again so that they approach true value, while values of potential states can be ignored -> on-policy performs better when branching factor is small and state set is large
### 7. Real-time dynamic programming
- a on-policy trajectory sampling version of the value iteration DP
- finding an optimal partial policy (policy that gives optimal actions for relevant states and arbitrary actions for irrelevant states), values of all states still have to be computed and (approximately) infinite amount of time for the value function to converge -> exploring start
- RTDP is proved to achieve optimal policy without under certain conditions
	1. the initial value of every goal state is zero
	2. there exist at least a path from starting state to goal state
	3. all reward for transitions to non-goal state are strictly negative
	4. initial value of each state is equal to or greater that their optimal values
-> stochastic optimal path problem, where our goal of reward maximization equals to cost minimization -> time-minimization control task
- example: Racetrack problem
	- RTDP needs half number of updates of DP to converge
		- converge criteria
			- for RTDP: time to reach goal for certain episodes (20) is stable
			- for DP: max change of value for all states is lower than a certain threshold ($10^{-4}$)
	- advantage of RTDP over DP: the optimal policy obtained for the converged value function, can be obtained long before the value iteration terminates -> by updating the greedy policy after each value iteration, DP algorithm is discovered to reach the optimal policy after half the number of steps needed for the optimal value function to converge
### 8. Planning at Decision Time
There are two types of planning. The type of planning we’ve discussed so far is called _background planning,_ in which we try to obtain the optimal the policy and value function using experience generated from a model. The other type of planning is called _decision-time planning,_ in which planning begins right after we encounter a particular state $S_t$ and ends when an action $A_t$ is selected. The simplest example of this type of planning is when we only have the state value function and try to evaluate each action possible from our current state to determine which action to take, though usually we tend to look deeper than that to examine different trajectories of next states and rewards. The policy and value obtain by this type of planning is particular to each state, so usually we discard them in problem with large state space where we rarely encounter the same state again.
### 9. Heuristic Search 
### 10. Rollout Algorithms
### 11. Monte Carlo Search Tree