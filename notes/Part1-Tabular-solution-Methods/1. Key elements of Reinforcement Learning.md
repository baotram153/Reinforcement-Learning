## 1. Reinforcement Learning
1. Reinforcement Learning là gì?
	- Find the best way to do sth (a way that maximize the overall reward) - a mapping from situation to action
2. Hai thuộc tính đặc trưng của Reinforcement Learning?
	- trial and error -> exploration vs exploitation problem
		- supervised learning: data with corresponding labels is provided
		- reinforcement learning: it is our main task to discover the labels to maximize a given goal
	- delayed reward 
		- what is the objective in supervised learning and unsupervised learning
			- supervised learning: find as many right labels for data as possible
			- unsupervised learning: find the underlying pattern / discover new insights about the data
		- what is the objective of reinforcement learning?
			- find a set of actions corresponding to situations that maximize the long-term reward
3. Reinforcement Learning khác Supervised Learning như thế nào?
	- Việc gắn nhãn cho từng trường hợp một cách chính xác và thu thập số lượng mẫu đủ nhiều để tổng quát được tất cả trường hợp mà agent có thể gặp phải là không thể
	- Nhiều bài toán chỉ biết goal cần đạt được chứ không biết được nhãn chính xác cho từng trường hợp cụ thể để đạt được goal đó. 
		- Ví dụ chơi cờ vua, bạn biết rằng mục tiêu của bạn là thắng được đối phương nhưng không biết nước đi cụ thể cho từng trường hợp, đặc biệt khi số trường hợp bạn có thể gặp phải lên đến $7.7\times10^{45}$ , và mục tiêu của bạn là checkmate (delayed reward) chứ không phải giành được nhiều điểm nhất có thể trong lượt đó (intermediate reward).
4. Reinforcement Learning khác Unsupervised Learning như thế nào?
	- khác về goal, mặc dù đều học từ data không gắn nhãn
## 2. Elements of Reinforcement Learning
1. policy
	- mapping from situations to action
2. reward
	- denotes which action is good and which action is bad
	- the goal of the agent is to maximize the overall reward over the long run
3. value function
	- calculate the **expected reward** the agent will recieve in the future, starting from that state
	- the notion of value is important since there are actions that have low reward but high value
		- think of human immediate and long term pleasure
	- the value is what determines the policy, but it is much more difficult to calculate than immediate reward and has to be continuously estimated and re-estimated over the entire lifetime of the agent
4. model
	- something that allow inference about how the environment will behave
	- used for planning - determine a course of action that takes into account possible future events without actually having experienced it (lên kế hoạch - yêu cầu cần có model về thế giới để biết được các sự kiện sẽ xảy ra nếu ta thực hiện một action cụ thể) -> model-based learning
		- mode-free learning: doesn't require planning, only explicit trial-and-error
## 3. Limitations and Scope
- the state signal is produced by some preprocessing system that is nominally part of the agent’s environment
- estimating value functions, but not the only way to solve reinforcement learning problems

## 4. Example: Tic-tac-toe
- “minimax” solution from game theory is not correct here because it assumes a particular way of playing by the opponent
- dynamic programming requires as input a complete specification of that opponent, including the probabilities with which the opponent makes each move in each board state
- evolutionary method: search the space of possible policies for one with a high probability of winning against the opponent
- temporal difference method
	- ![State-action Tree]("../images/Chapter1/state-action-tree.png")
	- $$V(S_t) \leftarrow V(S_t) + \alpha [V(S_{t+1}) - V(S_{t})]$$