### 5.1  Monte Carlo Prediction
- obvious idea: average the returns offered after visits to that state $\rightarrow$ as the number of returns increases, the average should converge to expected return
- first-visit MC and every-visit MC
	- suppose we have a set of episodes obtained by following $\pi$ and passing through $s$ and want to estimate $v_\pi(s)$
	- we call each passing through $s$ is a ***visit*** to $s$
		- first-visit MC: estimate $v_\pi(s)$ by averaging out returns followed by first visit of $s$ in each episode
		- every-visit MC: estimate $v_\pi(s)$ by averaging out returns followed by every visit of $s$ in every episode
- ![[Pasted image 20230920164644.png]]

### 5.2 Monte Carlo Estimation of Action Values
- without a model of the environment, it's useful to estimate action value (value of each state-action pair) instead of just state value
	- when a model is available, the state value function itself is sufficient to determine a better policy: we only need to look one step ahead to choose the action that leads to a better combination of immediate reward and expected return of the next state (DP)
	- but without a model of the environment, state value itself is not sufficient
- estimation of action value is the same as of state value, instead of now we link each visit to a state-action pair rather than to a state
- problem: some action-state cannot be visited if our policy is a deterministic one
	- deterministic policy $\rightarrow$ the agent only chooses a particular action when encounters each state $\rightarrow$ only receive and average returns from a state-action pair $a,s$ from state $s$ $\rightarrow$ estimation for other actions are not improved over experience
	- the goal of rl is to choose the best action among possible actions $\rightarrow$ need to estimate value of all actions
- idea for solutions
	- specifying that an episode start from a certain state-action pair, and ensure that every state-action pair has non-zero propability of being chosen in the start
	- consider only policies that are stochastic, with a non-zero possibility of selecting all actions in each state
### 5.3 Monte Carlo Control
- ![[Pasted image 20230920213805.png]]
- policy evaluation is done exactly as described in the preceding section
- policy improvement $$\pi(s) = \arg \max_a q(s,a)$$
- proof of policy improvement theorem $$\begin{align} q_{\pi_k}(s,\pi_{k+1}(s)) & = q_{\pi_k}(s, \arg \max_a q_{\pi_k}(s,a)) \\ & = \max_a q_{\pi_k}(s,a) \\ & \geq q_{\pi_k}(s,\pi_k(s)) \\ & = v_{\pi_k}(s) \end{align}$$
	- assure that if we choose $\pi_{k+1}(s)$ as the new policy according to above equation, $\pi_{k+1}(s)$ is always better than $\pi_k(s)$
- 2 unlikely assumptions that need to get rid of to obtain a practical algorithm
	- exploring start $\quad (1)$
	- infinite number of episodes $\quad (2)$
- solution for $(2)$
	1. try to approximate $q_{\pi_k}(s)$ in each policy evaluation
		- measurements are taken to obtain bounds on the magnitude and probability of error in the estimates
		- sufficient steps are made to assures that these bounds are small enough
		$\rightarrow$ require big number of episodes
	2. give up trying to complete policy evaluation before moving to policy improvement
		- value iteration
		- in-place policy iteration
- for Monte Carlo method it's usual to do policy evaluation and policy improvement on an episode-to-episode basis. meaning after each episode
	- state-action pairs that appear in each episode is approximated for returns
	- policy is improved at each state $s$ that appears in that episode
- ![[Pasted image 20230920231624.png]]
### 5.4  Monte Carlo Control without Exploring Start
- 2 approaches: on-policy and off-policy
- on-policy
	- soft policy: policy with $\pi(a | s) > 0 \quad \forall a \in \mathcal A \text{ and } \forall s \in \mathcal S$
	- $\epsilon$-soft policy: policy with $\pi(a | s) > \frac{\epsilon}{|\mathcal A(s)|} \quad \forall a \in \mathcal A \text{ and } \forall s \in \mathcal S$
	- $\epsilon$-greedy policy: 
		- an example of $\epsilon$-soft policy
		- ungreedy actions are selected with probability $\frac{\epsilon}{|\mathcal A(s)|}$ 
		- greedy action is selected $1-\epsilon + \frac{\epsilon}{|\mathcal A(s)|}$
		- intuition: most of the time the action that gives maximal action value is selected, other times other non-greedy actions are selected randomly
	- algorithm ![[Pasted image 20230921131136.png]]
	- prove that $q_\pi(s,\pi'(s)) \geq v_\pi(s)$ (condition for policy improvement theorem) $$\begin{align} q_\pi(s,\pi'(s)) & = \sum_a \frac{\epsilon}{|\mathcal A (s)|}q_\pi(s,a) + (1-\epsilon)\max_aq_\pi(s,a) \\ 
	 & = \sum_a\frac{\epsilon}{|\mathcal A (s)|}q_\pi(s,a) + (1-\epsilon) \sum_a \frac{\pi(a|s) - \frac{\epsilon}{|\mathcal A (s)|}}{1-\epsilon} q_\pi(s,a) \qquad (1)\\ 
	 & = \sum_a \pi(a|s)q_\pi(s,a) \qquad (2) \\
	 & = v_\pi(s)\end{align}$$
		 - on line $(1)$, the sum of $q_\pi(s,a)$ over $a$ is weighted so that sum of those weight is 1: $$\begin{align} \sum_a \frac{\pi(a|s) - \frac{\epsilon}{|\mathcal A (s)}}{1-\epsilon} & = \frac{\sum_a \pi(a|s) - \sum_a \frac{\epsilon}{|\mathcal A (s)|}}{1 - \epsilon} \\
		 & = \frac{1 - \frac{\epsilon}{|\mathcal A (s)|} \cdot |\mathcal A(s)|}{1 - \epsilon} \\
		 & =  1
		 \end{align}$$

### 5.5  Off-policy Prediction via Importance Sampling
- dilemma: we want a policy that will optimize the action of the agent, but still be explorative so that possible actions can be examined before choosing the optimal one
	- on-policy methods enable this exploration, but the policy we obtain is not really optimal since the greedy action is chosen with probability of $1-\epsilon + \frac{\epsilon}{|\mathcal A (s)|}$
- solution: have 2 policy
	- behavior policy: policy agent uses to generate learning data
	- target policy: policy that the agent try to optimize
	$\rightarrow$ we expect that our target policy will be deterministic optimal policy while the behavior policy remain stochastic and more exploratory (like $\epsilon$-greedy policy)
- off-policy methods are often of greater variance and slower to converge, but is more powerful and general: on-policy methods are off-policy methods with target policy and behavior policy being the same
- requirement
	- every action taken in $\pi$ has to be taken, at least occationally, in $b$ ($\pi(a|s)>0$ implies $b(a|s)>0$ ) $\rightarrow$ the assumption of coverage
- problem:
	- because the returns we receive following $b$ has different expectation compared to $\pi$: $\mathbb E_b[G_t| S_t = s] = v_b(s)$, so they cannot be averaged to obtain $v_\pi(s)$
- solution
	- importance sampling: weighting returns base on the ***relative probability of their trajectory*** according to the 2 policies
	- importance-sampling ratio: 
		- probability of the trajectory $S_t, A_t, S_{t+1}, A_{t+1}, ..., S_{T-1}, A_{T-1}, S_{T}$ 
	$$\begin{align}
	& Pr\{S_t, A_t, S_{t+1}, A_{t+1},..., S_T|S_t, A_{t:T-1} \sim \pi \} \\
	= & \pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})...\pi(A_{t-1}|S_{t-1})p(S_t|S_{t-1},A_{t-1}) \\
	= & \prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1}|S_k, A_k)
	\end{align}$$
		- thus the relative probability of that same trajectory but follow 2 different policy $$\rho_{t:T-1} = \frac{\prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} b(A_k | S_k)p(S_{k+1}|S_k, A_k)} = \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)}{\prod_{k=1}^{T-1}b(A_k|S_k)}$$
		- because the dynamics of the environment is the same no matter which policy we follow, the terms $\prod_{k=t}^{T-1}p(S_{k+1}|S_k, A_k)$ are cancelled out 
	- the ratio $\rho_{t:T-1}$ transform the returns to have the right expected value $$\mathbb E_\pi[G_t|S_t = s] = \mathbb E_b[\rho_{t:T-1}G_t|S_t=s]$$
		- ordinary importance sampling $$v_\pi(s) = \frac{\sum_{t \in \mathcal J(s)}\rho_{t:T(t)-1}G_t}{|\mathcal J(s)|}$$
		- weighted importance sampling $$v_\pi(s) = \frac{\sum_{t \in \mathcal J(s)}\rho_{t:T(t)-1}G_t}{\sum_{t \in \mathcal J(s)}\rho_{t:T(t)-1}}$$

### 5.6 Incremental Implementation
- on-policy MC methods and off-policy MC methods that use ordinary importance sampling, the incremental implementation used in chapter 2 can be reused, but this time for averaging returns (on-policy) or averaging weighted returns (off-policy using ordinary importance sampling)
- off-policy MC methods that use weighted importance sampling
	- Suppose that we have a sequence of returns that start from one state $G_1, G_2,..., G_{n-1}$ with their corresponding weights $W_i (i: 1 \rightarrow n-1)$ $$V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}$$
	- Updating formula $$\begin{align} V_{n+1} & = V_n + \frac{W_n}{C_n}(G_n - V_n) \\
	\text{and} \qquad C_n & = C_{n-1} + W_n\end{align}$$
		- proof $$\begin{align} V_{n+1} & = \frac{\sum_{k=1}^{n-1}W_kG_k + W_{n}G_n}{\sum_{k=1}^{n-1}W_k}\cdot\frac{\sum_{k=1}^{n-1}W_k}{\sum_{k=1}^{n}W_k} \\
		 & = V_n \cdot \frac{\sum_{k=1}^{n-1}W_k}{\sum_{k=1}^{n}W_k}  + W_nG_n \cdot \frac{1}{\sum_{k=1}^{n}W_k} \\
		 & = V_n \left( 1 - \frac{W_n}{\sum_{k=1}^{n}W_k}\right) + G_n\frac{W_n}{\sum_{k=1}^{n}W_k} \\
		 & = V_n + \frac{W_n}{\sum_{k=1}^{n}W_n}(G_n-V_n)
		\end{align}$$
		- with $C_n = W_n + C_{n-1}$ and $C_0 = 0$, we have the updating formula
	- ![[Pasted image 20230922232833.png]]

### 5.7 Off-policy Monte Carlo control
- ![[Pasted image 20230923082100.png]]
	- Why update $W$ by $\frac{1}{b(A_t|S_t)}$ but not $\frac{\pi(A_t|S_t)}{b(A_t|S_t)}$?
	- Why proceed to next episode when $\pi(A_t|S_t)$ is updated? 

