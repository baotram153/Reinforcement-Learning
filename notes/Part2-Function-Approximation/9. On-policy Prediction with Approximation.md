## 9.1  Value function approximation
- representation $s\mapsto \text{target value}$  
	- DP: $s \mapsto \mathbb{E}[R_{t+1} + \gamma \hat v(S_{t+1}, w)|S_t = s]$ 
	- MC: $S_t \mapsto G_t$
	- TD: $S_t \mapsto R_{t+1} + \gamma \hat v(S_{t+1}, w)$
- methods used in RL must
	- be able to handle incremental acquired data
	- handle nonstationary target functions
## 9.2  The Prediction Objective
- in tabular method, there's no prediction objective since the value can be exactly estimated for each state
- in approximation method, making one state more accurate meaning making other states less accurate since the number of weights is smaller than the number of states $$\overline{VE} = \sum_{s \in \mathcal S}\mu(s)\left[\hat v(s,\boldsymbol{w}) - v_\pi(s)\right]^2$$
	- $\mu(s) \geq 0$: how important we consider that state, $\sum_s \mu(s) = 1$
	- ~~vì sao $v_\pi(s)$ nhưng $v(s,w)$ không có $\pi$~~
- often $\mu (s)$ is chosen to be the fraction of time spent in $s$
	- in on-policy training, this is called on-policy distribution
- on-policy distribution
	- in continuous task
	- in non-continuous task:
		- time spent in $s$ = number of episodes start with $s$ + number of times $s$ is transitioned to
		- $h(s)$: probability that an episode start with $s$
		- $\eta(s)$: number of time step spent on average in $s$ in a single episode
			- $$\eta(s) = h(s) + \sum_{\overline s\in \mathcal S}\eta(\overline s)\sum_{a \in \mathcal A}\pi(a|\overline s) p(s|\overline s, a)$$
			- ~~why don't take the $\eta(s)$ directly~~
		- $$\mu(s) = \frac{\eta(s)}{\sum_{s'} \eta(s')}$$
- an ideal goal is to reach global optimum, but for complex functions, local optimum is acceptable
- 2 processes: use the update of the former to generate training examples for the later
	- methods for value estimation
	- methods for function approximation
## 9.3  Stochastic-Gradient and Semi-Gradient methods
- Update parameters
	- $$\begin {align} \boldsymbol w_{t+1} & = \boldsymbol w_{t}-\frac{1}{2}\alpha\nabla\left[v_\pi(s) - \hat v (s,\boldsymbol w)\right]^2 \\
	& = \boldsymbol w_{t} + \alpha \left[v_\pi(S_t) - \hat v(S_t, \boldsymbol w_t) \right] \nabla \hat v(S_t,\boldsymbol w_t) 
	\end{align}$$
	- gradient $\nabla f(\boldsymbol w)$ of a scalar expression $f(\boldsymbol w)$ that is a function on a vector $\boldsymbol w \in R^d$ is  a vector of $d$ elements, each component is the partial derivative of the scalar expression with respect to each component of the vector $\boldsymbol w$
		- $$\nabla f(\boldsymbol w) = \left( \frac{\partial f(\boldsymbol w)}{w_1}, \frac{\partial f(\boldsymbol w)}{w_2}, ..., \frac{\partial f(\boldsymbol w)}{w_d}\right)^\top$$
- For the formula to work, it is required that $v_\pi(s)$ is a fixed point (which isn't attainable since we have to continuously estimate $v_\pi(s)$), or at least the approximation of $v_\pi(s)$ ($U_t$) is not biased, meaning $\mathbb E[U_t | S_t = s] = v_\pi(s)$
	- $$w_t = w_{t-1} + \alpha \left[ U_t - \hat v(s, \boldsymbol w)\right]\nabla \hat v(s,\boldsymbol w)$$
- This work for MC methods, since target value in this case is $G_t$ whose expectation, following the definition of state value, is $v_\pi(s)$ 
	- ![[Pasted image 20231029102630.png]]
- But some other algorithms cannot assure the condition (TD(0): $U_t = R_{t+1} + \gamma \hat v_(S_{t+1}, \boldsymbol w_t)$ or DP: $U_t = \sum_a \pi(a|S_t) \sum_{s',r} p(s', r|S_t, a)[r + \gamma \hat v(s', \boldsymbol w_t)]$)
	- notice that if we use above expression for $U_t$ then the gradient calculation does not hold, since $U_t$ also depends on $\boldsymbol w_t$ -> gradient calculation only take into account the effect of changing $\boldsymbol w$ on the old value, not the target value
	- convergence of predicting algos ![[Pasted image 20231118164852.png]]
	- but $\boldsymbol w$ can still convert in some important cases
	- ![[Pasted image 20231029104523.png]]
	- pros: faster training, enable training to be fully online, without waiting for the end of the episodes
- States agregation: 
	- a simple form of generalizing function approximation
	- states are group together for 1 estimated value (one component of the weight vector), each change in state value of each group leads to the change of the component of the whole group
	- gradient $\nabla \hat v (S_t,\boldsymbol w_t)$ is 1 for $S_t$'s group's component and 0 for other components 
	- Example: states aggregation in 1000-random walks
		- ![[Pasted image 20231029110037.png]]
		- state that have the highest probability to occur is state 500 since all episodes start from that state, next are states near to the middle (499 and 501)
		- there's a huge shift in approximation of $\hat v$ in leftmost and rightmost groups since the difference in $\mu(s)$ is huge among states in those groups 
## 9.4   Linear methods
- function to approximate has the form $$\hat v(s,\boldsymbol w) = \boldsymbol w^\top \boldsymbol x(s) = \sum_{i=1}^dw_ix_i(s)$$
	- $\boldsymbol x(s)$: feature vector of state $s$, each component is value of a function $x_i(s): \mathcal S \rightarrow \mathbb R$ (mỗi state có một feature vector riêng)
- update of $\boldsymbol w$ under linear function approximate
	- $$w_{t+1} = w_t + \alpha \left[U_t - \hat v (s,w_t)\right]x_t$$
- can $\boldsymbol w$ converges if using TD(0) update (semi-gradient update) under linear function approximation?
	- update for $w$ in TD(0) $$\begin {align} w_{t+1} & = w_t+\alpha \left[ R_{t+1} + \gamma \hat v(S_{t+1},w_t) - \hat v(S_t, w_t) \right]x_t \\
	& = w_t + \alpha \left[ R_{t+1} + \gamma w_t^\top x_{t+1} - w_t^\top x_t \right]x_t \\
	& = w_t + \alpha \left[ R_{t+1}x_t - x_t(\gamma x_{t} - x_{t+1})^\top w_t \right]
	\end{align}$$
	- đặt $A = \mathbb E[x_t(\gamma x_{t} - x_{t+1})^\top]$, $b = \mathbb E[R_{t+1}x_t]$, ta có $$\mathbb E[w_{t+1}|w_t] = w_t + \alpha[b-Aw_t]$$ 
 
		- $w$ hội tụ $\Leftrightarrow$ $$\begin{align} b-Aw_t & = 0 \\
		w_t & = A^{-1}b \\
		\end{align}$$
		- chứng minh $w$ hội tụ
	- it is also proven that, $\overline {VE}$ is bounded within the extension of the possible minimum error
		- $$\overline {VE} (w_{TD}) \leq \frac{1}{1-\gamma}\min_w\overline{VE}(w)$$
- other critical condition
	- reward
	- feature
	- decrease in the step-size parameter
	- state value are updated according to on-policy distribution
- performance of linear function approximation using TD updates
	- ![[Pasted image 20231029213027.png]]
	- worse than MC value approximation
	- average RMS error over number of bootstraping steps is strikingly similar to tabular method
- algorithm for value function approximation using TD(0)
	- ![[Pasted image 20231029213808.png]]
## 9.5   Feature Construction for Linear Methods
- motivation: linear function cannot makes good use of the relationship between different features if they are coded separately
### 9.1   Polynomials
- suppose each state in our state space has $k$ dimensions $s_1, s_2, ..., s_k$, an $n$-order polynomial basis feature $x_i(s)$ of each state can be written as $$x_i(s) = \prod_{j=1}^ks_j^{c_{i,j}}$$
	- each $c_{i,j}$ is an integer of the set ${0, 1, ..., n}$
	- we have $(n+1)^k$ features! normally a subset of features is chosen based on prior knowledge
### 9.2   Fourier Basis
- motivation: any function can be approximated using enought weighted sine / cosine basic functions
- $$x_i(s) = \cos(\pi s^\top c^i)$$
	- with $\textbf{c}^i = (c^i_1, c^i_2, ..., c^i_k)$ with $c^i_j \in \{1, 2, ..., n\}$ 
- ![[Pasted image 20231119000433.png]]
- ![[Pasted image 20231119000517.png]]
- step size when using Fourier basis should be set different for each feature $$\alpha_i = \frac{\alpha}{\sqrt{(c_1^i)^2 + (c_2^i)^2 + ... + (c_k^i)^2}}$$

### 9.3  Coarse Coding
- ![[Pasted image 20231119092021.png]]
- each feature is a circle, a state has certain features if it's in the circle represents each feature
- correspond with each circle is a weight $w_i$, if we train at one state (a point in the space), all the circles that intersect at that point are affected
- example ![[Pasted image 20231119092613.png]]
### 9.4   Tile Coding
- ![[Pasted image 20231119092737.png]]
- left: state aggregation (activeness of each feature corresponds to a group of states)
- right: a state falls into 4 tiles, each tile represent the active feature of that state
- why assymetrical offsets are preferred than symmetrical offset? ![[Pasted image 20231119100438.png]]
- How to choose tiling offset / number of tiling / shape of the tiles?
	- offset: displacement vector consists of first odd integers
	- n set to number of 2 greater than or equal to 4k
	- ![[Pasted image 20231119101850.png]]
### 9.5   Radial Basis
- $$x_i(s) = \exp \left( -\frac{||s-c_i||^2}{2\sigma^2} \right)$$
- ![[Pasted image 20231119103655.png]]
## 9.7   ANN
- Review
	- NN that has at least 1 loop is recurrent NN, RL uses both feedforward and recurrent NN
	- at every unit, all previous units are weighted and summed up (calculate a linear function over input units) -> put through an "activation" (non-linear) function
		- ReLU
		- binary function that results 1 if output > $\theta$
		- Simoid-like
	- though a 1-hidden layer nn with finitely big enough hidden units is capable of representing any function (theoretically), in practice, a nn with many hidden layer with much smaller number of hidden units in each layer is easier and more effective in representing complex function (?)
	- problems
		1. overfitting
			- overfitting: modify objective function (put restriction on the scale of parameters / increase relationship between many parameters) -> reduce degree of freedom
			- early stopping (when performance on validation set decreases)
			- drop out
		1. vanishing and exploding gradient
	- deep belief network (?)
		- deepest layer is trained using unsupervised learning -> extract relationships from input features
		- next deepest layer is trained using parameters from trained layer as input and so on
		- parameters in this DBN are then used as initial parameters for our supervise learning task -> more efficient than random initialization of parameters -> suggested cause: place the network in parameter space where backprop can make good progress
	- batch normalization: scale the inputs in each batch to have mean 0 and unit variance -> suggested cause: internal covariance shift / increase the smoothness of the optimization landscape
	- deep residual learning: 
		- a deeper nn is effective, but easily to lead to vanishing or exploding gradient -> skip connection to connect input to output, allow the flow to "skip" certain layer -> residual blocks
		- theoretically, a residual block always makes the network "better"
	
- RL related
	- chapter 15 discuss NN training with reinforcement learning principles instead of backpropagation -> less efficient than backprop, but representative for how nn learn

## 9.8  Least-Squared TD
- Idea: Computing $w_{TD}$ iteratively is a wasteful of data -> estimate $\hat A$ and $\hat b$, then directly compute the TD fixed point
- $$\hat A_t = \sum_{k=0}^{t-1}x_k(x_k - \gamma x_{k+1}) + \epsilon I \quad \text{and} \quad \hat b_t = \sum_{k=0}^{t-1}R_{k+1}x_k$$
	- $\epsilon I$ ensures that $\hat A_t$ is always invertible 
- Computation complexity
	- TD(0) requires memory and per step computation of only $\mathcal O(d)$
	- an update of $\hat A$ requires an outer product (a column vector multiply a row vector) which requires computation and memory of $\mathcal O(d^2)$
	- the final update requires inverse of $\hat A_t$ -> $\mathcal O(d^3)$
	- howerver since $\hat A_t$ is in special form, the inverse of $\hat A_t$ can be updated incrementally with computation complexity $\mathcal O(d^2)$
- ![[Pasted image 20231119111050.png]]
## 9.9   Memory Based
- Parametric approach: examples are used for parameter update -> thrown away -> query state's value is calculated using updated parameters
- Non-parametric approach: examples are stored in memory -> query state's value is calculated based on stored examples -> lazy learning
- Different memory based methods depend on:
	- How the stored training examples are selected
	- How they are used to respond to a query
- Methods
	- nearest neighbor
	- weighted average
	- locally weighted regression
- Advantages
	- not limit approximation to some pre-defined function
	- allow the update to only affect local states
	- avoid curse of dimensionality: for a state space with k dimensions, tabular method require memory exponential in k, while memory based methods only require memory propotional to k for each state, and memory as a linear function of n for n examples. nothing is exponential in k or n
- Disadvantages
	- how long is needed to calculate query state's value?
		- solution: parallel computing or special purpose hardware / special multi-dimensional data structure to store the training data (k-d tree)
## 9.10   Kernel Based
- generally, weights do not have to depend on distance, they can depend on relationship between states $$k: \mathcal S \times \mathcal S \rightarrow R$$ where $k(s,s')$ is the weights about $s'$ on its influence on answering the query about $s$
- $$\hat v (s, \mathcal D) = \sum_{s' \in \mathcal D}k(s,s')g(s')$$
- weighted average method is a special case of kernel based method where $k(s,s')$ is defined by the distance between $s$ and $s'$
- any linear parametric regression method, with state represent by feature vectors $x(s)$, can be recast as kernel regression where $$k(s,s') = x(s)^\top x(s')$$
	- kernel regression that use this kernel function produce the same approximation as parametric method would if it used these feature vectors and is trained with the same data
- advantage: less complex (?)
## 9.11   Looking Deeper
- the update trajectory we follow is from ==on-policy distribution== (states that have high values are favored by the policy -> encountered more -> estimated more accurately)
- motivation: in function approx where there's trade off between accuracy in estimating different states, what if we want to estimate 1 state more accurately than others?
- idea
	- assign each state an interest $I_t$ from 0 to 1
	- calculate the emphasis $M$ for each state $$M_{t} = I_t + \gamma M_{t-n}$$
		- for Monte Carlo, $n = T-t$ and $M_t = I_t$ (?)
	- modified update function for $w_{t+n}$ $$w_{t+n} = w_{t+n-1} + \alpha M_t [G_{t:t+n} - \hat v(S_t, w_{t+n-1})]\nabla \hat v(S_t, w_{t+n-1})$$
		- $0 \leq t \leq T$
	- example ![[Pasted image 20231121104357.png]]
		- Monte Carlo
			- not using emphasis: $w = (3.5, 1.5)$
			- using emphasis: $w = (4, ?)$, $w_2$ will never be updated
		- TD(2)
			- not using emphasis $w = (3.5, 1.5)$
			- using emphasis: $w = (4, 2)$, third state has emphasis $M_3 = I_3 + \gamma^2 M_1 > 0$  

- $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + ... + \gamma^{T-t-1}R_T$$
## Unresolved
- có thể concate coarse coding và dimension vector đc k