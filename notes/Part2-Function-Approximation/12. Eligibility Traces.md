### Summary
- eligibility trace is a mechanism
- unify and generalize TD and Monte Carlo
- enable Monte Carlo to be implemented online and on continuing task
- idea
	- eligibility trace vector $z_t \in R^d$ along with parameter vector $w_t \in R^d$
	- whenever a component in $w_t$ is used to calculate the state / state-action value -> the corresponding component in $z_t$  will bump up and then fade away
	- trace decay parameter $\lambda \in [0,1]$ determin the rate in which the trace fade
- advantage
	- don't need to store $n$ feature vectors of $n$ states, just a single trace vector (?)
	- learning takes place continuously
## 1. The $\lambda$ return
- averaging: any update step can use average of multiple n-step returns, but the weights must be positive and add up to 1
	- example: 
		- $\frac{1}{2}G_{t:t+2} + \frac{1}{2}G_{t:t+4}$ 
		- we can use averaging to combine returns from TD and Monte Carlo, or returns sampled from experience and expected return from DP
- diagram for a compound update ![[Pasted image 20231129162847.png]]
	- the update is done when the longest of its component updates is done (in this case $G_{t:t+4}$) -> one may wants to restrict the length of the longest update to avoid the delay
- $TD(\lambda)$ algorithm
	- can be understood as one way of averaging n-step updates
	- ![[Pasted image 20231129171932.png]]
		- combination of $n-$step returns, each return has a weight proportional to $\lambda^{n-1}$, multiplied with $(1-\lambda)$ to assure the sum of weights to be equal to 1  
		- diagram of $TD(\lambda)$ ![[Pasted image 20231129172645.png]]
		- ![[Pasted image 20231129174725.png]]
		- after terminal state is reached, all returns after that are conventional return ($G_t$)
		- we can separate these returns from our main sum, yielding ![[Pasted image 20231129175558.png]]
			- proof $$\begin{align}
			G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t:t+n} + (1-\lambda)\sum_{n=T-t}^\infty\lambda^{n-1}G_t
			\end{align}$$
				- $$S = (1-\lambda)\sum_{n=T-t}^\infty \lambda^{n-1} G_t = (1-\lambda)(\lambda^{T-t-1} + \lambda^{T-t} + \lambda^{T-t+1}+...)G_t$$
				- đặt $A = \lambda^{T-t-1} + \lambda^{T-t} + \lambda^{T-t+1}+...$, ta được $$\begin{align}
				\frac{A}{\lambda} - A & = \lambda^{T-t-2} \\
				A & = \frac{\lambda^{T-t-1}}{1-\lambda}
				\end{align}$$
				- thay vào $S$ ta được $$S = \lambda^{T-t-1}G_t$$
		- nhận xét
			- $\lambda = 1$: the main sum disappeared, return we get is purely conventional return ($G_t$)
			- $\lambda = 0$: return is $G_{t:t+1}$ (one-step TD)
	- off-line $\lambda-$ return algo ![[Pasted image 20231129181519.png]]
		- off-line: at the end of episode, a sequence of updates is made according to semi-gradient rule, target is $\lambda-$return
	- performance of off-line $\lambda-$return algo vs $n-$step TD on 19-state random walk ![[Pasted image 20231129181914.png]]
- ![[Pasted image 20240112101319.png]]
## 2. TD($\lambda$)
- advantages
	- update at multiple times in an episode instead of only at terminal state -> faster learning
	- computation is distributed equally through time
	- can be applied for continuous problems
- eligibility trace vector $z_t \in R^d$ ($d$ is the number of dimesions of $w$)
	- $z_{-1} = 0$ (initialized to be zero)
	- $z_{t} = \gamma \lambda z_{t-1} + \nabla \hat v(S_{t}, w_t)$	
	- update parameter $$w_{t+1} = w_t + \alpha\delta_tz_t$$
	- eligibility trace keeps track of the component in parameter vector that correspond to the most change in recent state valuations (in linear function approximation its the sum of past, fading input vectors) 
	- algo ![[Pasted image 20231129211530.png]]
- intuition
	- using eligibility trace, the error in current state not only be use to change the value of its preceding state, but the value of its multiple preceding states
	- ![[Pasted image 20231129211858.png]]
	- if $\lambda = 0$ -> TD(0)
	- if $\lambda = 1$ -> TD(1) -> the credit given by earlier states fall at the rate of $\gamma$ per-step. if $\lambda = 1, \gamma = 1$, the credit given by earlier state doesn't fade away -> like MC return with undiscounted reward
	- performance in 19-state random walk ![[Pasted image 20231129213756.png]]
	- error on continuing discounted case (?) ![[Pasted image 20231129214023.png]]
## 3. n-step Truncated $\lambda-$return methods
- The residual weight is given to the longest possible n-step return - $G_{t:h}$ instead of the conventional return, $G_t$![[Pasted image 20231129215315.png]]
- ![[Pasted image 20231129220418.png]]
- ![[Pasted image 20231129221925.png]]
- more efficient implementation for $G^\lambda_{t:t+k}$ (?) 
	- ![[Pasted image 20231129222016.png]]
	- where ![[Pasted image 20231129222038.png]]
## 12.4    Redoing updates: Online $\lambda-$ return algorithm
- trade off in choosing truncation parameter n 
- idea: after each transition to next state, we extend the horizon, and use the new experience to reassign the value of earlier states
- for example: 
	- start with an arbitrary parameter vector $w_0$, and the horizon of 1, after the first transition we get to next state $S_1$, gain reward $R_1$, the estimated return for the first state is $G_{0:1} = R_1 + \hat v(S_1, w_0)$, we then use this return to update the parameter to $w_1^1$
	- after the second transition, we have $S_2, R_2$, we can then use this info, plus the newly updated parameter $w_1^1$ to compute a more accurate value for each earlier state $G^\lambda_{0:2} = (1-\lambda)G_{0:1} +  \lambda G_{0:2}$, after using this return to update $w$ we obtain $w_1^2$, this parameter vector is then used to compute $G^\lambda_{1:2} = R_{2} + \hat v(S_2, w_1^2)$, update $w$ again we obtain $w_2^2$
- advantage: if computation complexity was not an issue, 
	- online $\lambda-$ return algorithm allows continuous update, starting from first stage
	- the parameter vector used in bootstrapping has more informative updates
- dùng $w_h^h$ ở sequence trước để tính target value, nhưng việc cập nhật $w$ bắt đầu từ $w_0$ (trong trường hợp episode ban đầu), hoặc $w^T_T$ của episode trước đó 
	- ![[Pasted image 20231129234511.png]]
		- "the first weight vector $w^h_0$ in each sequence is that inherited from the previous episode (so they are the same for all h)"
		- "at the final horizon $h = T$ we obtain the final weights $w_T^T$ which will be passed on to form the initial weights of the next episode"
- update in general form ![[Pasted image 20231129234641.png]]
- comparison: online $\lambda-$return performs a little bit better (the line $\lambda =0$ is the same for both algo) ![[Pasted image 20231130000257.png]]
## 12.5  True TD($\lambda$)
- derivation: van Seijen et al., 2016
- strategy
	- lay the parameters of each sequence in a row, we obtain a triangle. only the parameters on the diagonal is needed ![[Pasted image 20240112121614.png]]
	- the idea is to form an updating formula that only use $w^{t}_{t}$ to calculate $w^{t+1}_{t+1}$
- if this is done in linear function approximation ($\hat v(s, w) = w^\top x(s)$), the updating formula is $$w_{t+1} = w_t + \alpha\delta_tz_t + \alpha(w_{t}^\top x_t - w_{t-1}^\top x_t)(z_t - x_t)$$
	- in which
		- $x_t = x(S_t)$
		- $\delta_t = R_{t+1} + \gamma v(S_{t+1},w_t) - v(S_{t}, w_t)$
		- $z_t$ is defined by $z_t = \gamma \lambda z_{t-1} + (1-\alpha\gamma\lambda z_{t-1}^\top x_t)x_t$ 
- the per-step computational complexity remain $\mathcal O(d)$, the same with TD($\lambda$) ![[Pasted image 20240112124850.png]]
- dutch trace vs accumulating trace vs replacing trace
	- definition of replacing trace ![[Pasted image 20240112134009.png]]
	- replacing trace is a crude approximation to dutch trace
	- accumulating trace is usually used for non-linear function approximation, in which dutch trace cannot be used
## 12.6  Dutch Traces in Monte Carlo learning
## 12.7  Sarsa ($\lambda$)
- update formula
	- $$w_{t+1} = w_t + \alpha[G_t^\lambda - \hat q (S_t, A_t, w_t)]\nabla \hat q(S_t, A_t, w_t) \quad t = 0,...,T-1$$
- intuition: Traces in Grid World ![[Pasted image 20240112222131.png]]
- Forward view: Sarsa ($\lambda$) uses the same update rule for $TD(\lambda)$
	- $w_{t+1} = w_t + \alpha \delta_t z_t$
	- $\delta_t = R_{t+1} + \gamma \hat q(S_{t+1}, A_{t+1}, w_t) - \hat q(S_t, A_t, w_t)$
	- $z_{-1} = 0$
	- $z_t = \gamma \lambda z_{t-1} + \nabla \hat q (S_t, A_t, w_t)$
	- algorithm ![[Pasted image 20240112221922.png]]
	- performance: Mountain Car ![[Pasted image 20240112222203.png]]
- Backward view: Sarsa ($\lambda$)
	- algorithm ![[Pasted image 20240112223107.png]]
	- performance ![[Pasted image 20240112223200.png]]
## 12.8  Variable $\lambda$ and $\gamma$
- $\lambda_t = \lambda(S_t, A_t)$ and $\gamma_t = \gamma(S_t)$ (termination function)
- Generalize the definition of return ![[Pasted image 20240112224026.png]]
	- to assure the sum is finite, we require that $\prod_{k = t}^\infty \gamma_k = 0$ with probability 1 for all $t$
- the new state-based $\lambda-$ return can be written as 
	- return of state ![[Pasted image 20240112231407.png]]
	- return of state-action
		- Sarsa form ![[Pasted image 20240112231908.png]]
		- expected Sarsa form ![[Pasted image 20240112231934.png]]
			- where 
				- ![[Pasted image 20240112232439.png]]