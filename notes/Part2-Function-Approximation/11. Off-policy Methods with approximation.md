### Summary
- concept of learnability
- off-policy methods are pretty much harder to converge and doesn't have good theoretical and pratical results
- two difficulties faced when using off-policy methods with approximation
	- update target -> solved in tabular methods by importance sampling
	- update distribution -> can be solved by
		- importance sampling to warp update distribution into on-policy update
		- develop a new learning algorithm that ensures convergence
## 1. Semi-gradient Methods
- Per-step importance ratio $$\rho_t = \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$$
- Off-policy semi-gradient methods
	- Semi-gradient TD $$w_{t+1} = w_t + \alpha \rho_t \delta_t \nabla \hat v(S_t,w_t)$$
		- $\delta_t = R_{t+1} + \gamma \hat v(S_{t+1}, w_t) - \hat v(S_{t}, w_t)$ (episodic and discounted)
		- $\delta_t = R_{t+1} - \overline R + \hat v(S_{t+1}, w_t) - \hat v(S_t, w_t)$ (continuous and undiscounted)
	- Semi-gradient Expected Sarsa $$w_{t+1} = w_t + \alpha \delta_t \nabla \hat q(S_t, A_t, w_t)$$
		- $\delta_t = R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})\hat q(S_{t+1}, a, w_t) - \hat q(S_{t}, A_t, w_t)$ (episodic and discounted)
		- $\delta_t = R_{t+1} - \overline R + \sum_a \pi(a|S_{t+1})\hat q(S_{t+1}, a, w_t) - \hat q(S_t, A_t, w_t)$ (continuous and undiscounted)
		- there's no importance ratio in the update of $w_t$ since we've specified the state-action pair
	- Generalize for multi-step semi-gradient methods
		- n-step Sarsa $$w_{t+n} = w_{t+n-1} + \alpha \rho_{t+1}...\rho_{t+n} (G_{t:t+n}- \hat q(S_t, A_t, w_{t+n-1})) \nabla \hat q(S_t, A_t, w_{t+n-1})$$
			- $G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+n} + \gamma^{n} \hat q(S_{t+n}, A_{t+n}, w_{t+n-1})$ (episodic and discounted)
			- $G_{t:t+n} = R_{t+1} - \overline R + R_{t+2} - \overline R + ... + R{t+n} - \overline R + \hat q(S_{t+n}, A_{t+n}, w_{t+n-1})$ (continuous and undiscounted)
			- lấy tích từ $\rho_{t+1}$ đến $\rho_{t+n}$ vì ở đây ta đã xác định giá trị cặp state-action cần tính là $S_t, A_t$ nên không cần phải sample xác suất thực hiện $A_t$ khi ở trong $S_t$ nữa, ta chỉ quan tâm xác suất xảy ra các cặp state action $S_{t+1}-A_{t+1}, ..., S_{t+n}-A_{t+n}$
		- n-step backup tree  $$w_{t+n} = w_{t+n-1} + \alpha (G_{t:t+n}- \hat q(S_t, A_t, w_{t+n-1})) \nabla \hat q(S_t, A_t, w_{t+n-1})$$
			- $G_{t:t+n} = \hat q(S_t, A_t, w_{t+n-1}) + \sum_{k=t}^{t+n-1} \delta_k \prod_{i=t+1}^k \gamma \pi(A_i|S_i)$
## 2. Example of Off-policy divergence
- example of Off-policy divergence
	- ![[Pasted image 20231128233122.png]]
	- part of a larger MDP, reward = 0 when transit into second state
	- update $$\begin {align}
	w_{t+1} & = w_t + \alpha\rho_t(R_{t+1} + \gamma \hat v(S_{t+1}, w_t) - \hat v(S_t, w_t))\nabla \hat v(S_t, w_t) \\
	& = w_t + \alpha(\gamma 2w_t - w_t) = (1+\alpha (2\gamma-1))w_t
	\end{align}$$
	- $w_t$ diverges when $\gamma > 0.5$
	- the stability does not depend on step size, the step size just affects the speed of the convergence / divergence
- Bard's counter example
	- ![[Pasted image 20231128234832.png]]
	- the set of feature vectors is linearly independant (each vector cannot be rewrite as a combination of other vectors)
	- update function using Dynamic programming![[Pasted image 20231129001703.png]] (vì sao phải tính tổng tất cả state mà không cập nhật theo từng state)
	- parameter converges ![[Pasted image 20231129001809.png]]
- other counter-examples demonstrate divergence of Q-learning algo, which has the best convergence guarantees among all control methods (due to its greedy policy improvement?)
	- solution for Q-learning: use a behavior policy that is close to the target policy (is not theoretically proven, but hasn't been seen to diverges in practice)
- other solution: 
	- find the min value of $\overline {VE}$ at each iteration instead of just one small step toward the estimated target
		- would work if the set of feature vectors form a linearly independent set (?)
		- Tsitsiklis and Van Roy’s Counterexample
			- ![[Pasted image 20231128193923.png]]
			- $$\begin{align}
			w_{k+1} = \arg \min_w \overline {VE} = & \arg \min_w \sum_{s \in \mathcal S}(\hat v(s, w) - \mathbb E[R_{t+1} + \gamma \hat v(S_{t+1}, w | S_t = s)])^2 \\
			= & \arg \min_w[(w - \gamma2w_k)^2 + (2w - (1-\epsilon)\gamma2w_k)^2]
			\end{align}$$
			- đạo hàm bằng 0 ta được $$ \begin{align}
			2(w - \gamma2w_k) + 4(2w - (1-\epsilon)\gamma2w_k) & = 0 \\
			\Leftrightarrow w = \frac{(6-4\epsilon)\gamma w_k}{5}
			\end{align}$$
			 - $w$ will diverges if  $\gamma > \frac{5}{6-4\epsilon}$ and $w_0 \neq 0$
	- use function approximators that do not extrapolate observed targets - averagers (?)
 
## 3. Deadly Triad
1. Function approximation
	- cannot be gotten rid of
	- parametric function approximation is more preferred, non-parametric methods are either too weak (?) or too expensive
1. Bootstrapping
	- can be gotten rid of but will result in slower learning (property of a specific state is not observed and is not recognized upon revisiting?)
	- impairs problems where the state property is not well represented -> cause poor generalization
1. Off-policy methods
	- simply use Sarsa instead of Q-learning to free the behaviors from target policy
	- for some learning strategies Off-policy method is neccessary (parallel learning)
		- parallel learning: we predict results of events not only in terms of reward but other sensories as well
		- so we need multiple target policies (?), hence multiple behavior policies
- convergence can be achieved by giving up 1 of 3
## 4. Linear Value-function Geometry
![[Pasted image 20240106010125.png]]
- real value space that contains actual value of all possible state / state-action  functions -> too complex to be obtained
- value subspace that contains representable value functions by function approximation
- best value function is the result of the projection operator $\prod$ from the real value space to the value subspace
	- if we define  $$\Vert v(s) \Vert^2_\mu = \sum_s{\mu(s)v(s)^2 } $$
		then our $\overline{VE}$ can be rewritten as $$\overline {VE} = \Vert v_\pi - v_w \Vert^2_\mu$$
	- the projection operator is an operator takes an arbitrary value function and turns it into representable value function that's closest in our norm
		$$\Pi v = v_w$$ such that $$w = \arg \min_w \overline {VE} = \arg \min_w \Vert v - v_w \Vert^2_\mu$$
	- the projection matrix
- Bellman error
	- Bellman equation $$v_\pi = \sum_s \pi(a|s) \sum_{s',r} p(s',r|s,a)(r + \gamma v_\pi(s'))$$
	- if an approximated function is substituted for $v_\pi$, then the difference between left and right hand side is the Bellman error $$
	\begin{align}\overline \delta_w(s) & = \left( \sum_s \pi(a|s) \sum_{s',r} p(s',r|s,a)(r + \gamma v_w(s')) \right) - v_w(s) \\
	& = \mathbb E_\pi[R_{t+1} + \gamma  v_w(S_{t+1}) - v_w(S_t) | S_t = s]
	\end{align}$$
		- Bellman Error is the expectation of TD error
	- Bellman error vector: vector that contain errors of all states
	- Mean Square Bellman error: size of that vector, denote the overall error of the value function
	- there's a point in the value function subspace that minimize the Bellman error  - $\min \overline {BE}$, this point is difference from that of the $\overline {VE}$
	- Bellman operator $$(B_\pi v)(s) = \sum_s \pi(a|s) \sum_{s',r} p(s',r|s,a)(r + \gamma v(s'))$$ Bellman error rewritten as $$\overline\delta_w = (B_\pi v_w) - v_w$$
	- the Bellman error vector leads to a point outside of the subspace, we then use that point to compute for the Bellman error, which leads to another point outside the subspace -> eventually, that point becomes the optimal value function ($\overline \delta_\pi = \vec0$) $$B_\pi v_\pi = v_\pi$$
	- but in practice, a point out of the subspace is unrepresentable, it must be projected back into the subspace before the next iteration. The projected Bellman error vector is $\Pi \overline \delta_w$
	- the error of that projected vector $$\overline {PBE} = \Vert \Pi \overline \delta_w \Vert^2_\mu$$
	- we can always find a point that have zero $\overline {PBE}$, this is the TD fixed point $w_{TD}$ (?)
## 5. Gradient Descent with Bellman Error
- idea: harness the power of SGD to assure convergence -> find new objective function / error function
- naive residual-gradient algorithm (minimizing mean square TD error)
	- idea: the error is the **expected** square of the TD error ![[Pasted image 20231129103819.png]]
		- per-step update based on samples
		- from experience follows policy $b$ ![[Pasted image 20231129104417.png]]
	- naive: does not necessarily converges to desirable place
	- em đọc kỹ thì có vẻ sự khác biệt giữa công thức update dựa trên TDE và công thức update trong thuật toán semi-gradient TD khác nhau ở chỗ
		- ct này:
			- đạo hàm cả "target value"
			- intuition: tập trung vào việc minimize obj function -> thuần GD -> chắc chắn hội tụ
		- ct trong semi-gradient TD
			- chỉ đạo hàm "old value"
			- intuition: xem "target value" là giá trị cố định, cần dịch old value đến giá trị này -> trong thực tế target value bị bias nên không thuần GD -> có thể không hội tụ
	- counter example: A-split
		- ![[Pasted image 20231129110001.png]]
		- $\gamma = 1$
		- true value of $A  = \frac{1}{2},B = 1, C=0$
		- value found by minimizing $\overline {TDE}: A=\frac{1}{2}, B=\frac{3}{4}, C=\frac{1}{4}$
		- mean error of 2 transitions using values found: $\frac{1}{16}$ 
			- first transition: $0.5 \times (0 + \frac{3}{4} - \frac{1}{2})^2 + 0.5\times(0 + \frac{1}{4} - \frac{1}{2})^2 =\frac{1}{16}$
			- second transition: $0.5 \times (1 + 0 - \frac{3}{4})^2 + 0.5 \times (0 + 0 - \frac{1}{4})^2 = \frac{1}{16}$
		- mean error of 2 trainsitions using true values: $\frac{1}{8}$ (first: $\frac{1}{4}$, second: $0$)
			- first transition: $0.5 \times (0 + 1 - \frac{1}{2})^2 + 0.5\times(0 + 0 - \frac{1}{2})^2 =\frac{1}{4}$ 
			- second transition:
- residual-gradient algorithm (minimizing Bellman Error)
	- ![[Pasted image 20231129112745.png]]
	- difference: make two samples for $S_{t+1}$ instead of one, make sense if we use simulated environemnt
	- assured to converge
- 3 problems of residual-gradient algo
	1. slow learning: is often combined with semi-gradient (first phase: semi-gradient for fast learning, second phase: residual-gradient algo for convergence assurance)
	2. still naive (?) (counter example: A-presplit)
		- ![[Pasted image 20231129112915.png]]
		- because all state transitions are deterministic, the values obtained by non-naive residual-gradient algo are the same as the naive one
	3. not learnable
## 6. The Bellman Error is not learnable
- definition of "learnable": to learn efficiently (within polynomial, not exponential number of examples)
- many features in reinforcement learning is not learnable, meaning that they can be computed given the internal structure of the environment, but cannnot be computed just from experience (học được nếu hiểu cơ chế môi trường cho reward, không học được từ chuỗi các state và reward nhận được)
- example: Markov Reward Process
	- ![[Pasted image 20231203084753.png]]
	- left MRP: 1 state, probability for receiving each of the 2 rewards is 0.5
	- right MRP: 2 states, possibility of receiving reward 0 / 2 at each state is deterministic, but since each state is equally likely to appear, probability for receiving each of the 2 rewards is still 0.5
	- $\gamma = 0$
	- optimal $w$ for two state is 1, this param yields $\overline {VE}$ of
		- left MRP: 0
		- right MRP: $0.5 \times (1-0)^2 + 0.5 \times (1-2)^2 = 1$
	- since the reward distributions of 2 MRPs are the same, we expect that $\overline {VE}$ for the same $w$ will be equal, but this is not the case $\rightarrow$ $\overline{VE}$ is not learnable
- $\overline {VE}$ is not learnable but the parameters that optimizes $\overline {VE}$ are
	- mean square return error (learnable?) $$\begin{align}
	\overline {RE}(w) & = \mathbb E[(G_t-\hat v(S_t, w))^2] \\
	& = \overline {VE}(w) + \mathbb E[(v_\pi(S_t) - G_t)^2] \qquad (1)\\
	\end{align}$$
	- the 2 objectives are the same except for the variance that doesn't depend on parameter $w \rightarrow$ they must have the same optimal parameter $w^*$
	- proof for $(1)$
- both $\overline {BE}$ and the parameters that optimizes $\overline {BE}$ are not learnable
		- counter-example: observable data distribution is the same for 2 MRPs ![[Pasted image 20231203100751.png]]
	- $w = (0, 0)^\top$ is the exact solution for 2 MRPs
		- left MRPs: $\overline {BE} = 0$
		- right MRPs: $\overline {BE} = \mu(B)1 + \mu(B')1 = \frac{2}{3}$
		$\rightarrow \overline {BE}$ is not learnable
	  - is the optimal $w$ learnable?
		  - left MRPs: $w$ that minimize $\overline {BE}$ is $(0,0)^\top$ for any $\gamma$
		  - right MRPs: $w$ that minimize $\overline {BE}$ is approaching $(-\frac{1}{2},0)^\top$ for $\gamma \rightarrow 1$
			  - $\frac{1}{3}\times(\frac{1}{2})^2 + \frac{2}{3}(0.5 \times (1 - \frac{1}{2}) + 0.5 \times(-1 + 0) - 0)^2 = \frac{11}{32}$ 
			  - $\frac{1}{3}\times(0 + \frac{1}{2})^2 + \frac{1}{3}(1 - \frac{1}{2} - 0)^2 +\frac{1}{3}(-1)^2 = \frac{1}{2}$ 
		  - why $w_1$ is $-\frac{1}{2}$ 
		  $\rightarrow w$ is not learnable
- $\overline {PBE}$, $\overline {TDE}$ are learnable in terms of both value and parameters
- tổng kết về learnability của các error value và optimal parameter :3 ![[Pasted image 20231203104002.png]]
## 7. Gradient-TD methods (assuming linear func approx)
- in linear case, the method of finding TD fixed point $w_{TD}$ (LSTD) has complexity $\mathcal O(d^2)$, we want to find a method with complexity $\mathcal O(d)$ but still assures convergence
- $$\begin{align}
\overline {PBE} & = \left \Vert \Pi \overline \delta_w \right \Vert^2_\mu \\
& = (\Pi \overline \delta_w)^\top D (\Pi \overline \delta_w) \\
& = \overline \delta_w^\top \Pi^\top D \Pi \overline \delta_w \\
& = \overline \delta_w^\top D X(X^\top DX)^{-1}X^\top D \overline \delta_w \quad (\Pi^\top D \Pi = D X(X^\top DX)^{-1}X^\top D) \\
& =(X^\top D \overline \delta_w) (X^\top DX)^{-1}(X^\top D \overline \delta_w)
\end{align}$$
- Hence $$\nabla \overline {PBE} = 2\nabla(X^\top D \overline \delta_w)^\top(X^\top D X)^{-1}(X^\top D \overline \delta_w)$$
- ...
- $$\nabla \overline {PBE} = 2\mathbb E[\rho_t(\gamma x_{t+1} - x_t)x^\top] \mathbb E[x^\top x]^{-1} \mathbb E[p_t\overline\delta x_t]$$
## 8. Emphatic-TD methods
## 9. Reducing variant

## Read later
- advantage value actor critic