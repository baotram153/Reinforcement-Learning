## 3.1 Advantages of Policy Gradient Methods
- concepts
	- $\theta \in R^{d'}$ : vector of policy parameters, $w \in R^d$ : vector of value function parameters
	- $J(\theta)$ : scalar performance (how is this measured) -> gradient ascent: $\theta_t = \theta_t + \alpha \widehat {\nabla J(\theta_t)}$
	- form a parameterized preference $h(s,a,\theta)$ for each state-action pair -> use *soft-max in action preference* to choose the best action $$\frac{e^{h(s,a,\theta)}}{\sum_be^{h(s,b,\theta)}}$$ 
- advantages
	1. an approximate policy can reach a deterministic policy, whereas $\epsilon-$greedy policy cannot
		- $\epsilon-$greedy always has $\epsilon$ probability of choosing a random action
		- using softmax on action-state values is not a good idea since each state-action pair will converge to their true value, which differ by a finite amount -> their probabilities of being chosen don't differ alot
		- whereas in policy gradient, the preference of each state-action doesn't have any specific value but a mean to choose the best action for each state 
	2. enable selection of actions with arbitrary probabilities
		- example: short corridor ![[Pasted image 20240103195450.png]]
			- the problem is difficult because all the states appear identical under the function approximation
	3. problems vary in the complexity of their action-state value or policy -> should use policy gradient in problems where policy is simple to estimate
	4. should use policy gradient if we want to inject our prior knowledge about the policy
## 13.2   Policy Gradient Theorem
- $$\nabla J(\theta) \propto \sum_s\mu(s)\sum_a q_\pi(s,a)\nabla\pi(a | s,\theta)$$
## 13.3  REINFORCE: Monte Carlo Policy Gradient
- $$\begin{align} \nabla J(\theta) & = \sum_s\mu(s)\sum_aq_{\pi}(s,a)\nabla\pi(a|s,\theta) \\
	& = E_{\pi}\left [\sum_aq_\pi(S_t, a) \nabla \pi(a|S_t,\theta) \right] \end{align}$$
	- update formula $$\theta_{t+1} = \theta_t + \alpha\sum_aq_\pi(S_t, a | w_t) \nabla \pi(a|S_t,\theta_t)$$
	- all-actions method -> promising and deserve further study
- $$\begin{align} 
E_{\pi}\left [\sum_aq_\pi(S_t, a) \nabla \pi(a| S_t,\theta) \right] & = E_\pi\left[ \sum_a\pi(a |S_t, \theta )q_\pi(S_t, a) \frac{\nabla\pi(a | S_t, \theta)}{\pi(a| S_t, \theta)} \right] \\
& = E_\pi\left[q_\pi(S_t, A_t)\frac{\nabla \pi(A_t| S_t,\theta)}{\pi(A_t | S_t, \theta)} \right] \\
& = E_\pi\left[G_t\frac{\nabla \pi(A_t| S_t,\theta)}{\pi(A_t | S_t, \theta)} \right]
\end{align}$$
	- update formula of REINFORCE algorithm $$\theta_{t+1} = \theta_t + \alpha G_t\frac{\nabla \pi(A_t| S_t,\theta_t)}{\pi(A_t | S_t, \theta_t)} $$
	- intuition
		- the gradient vector is the direction in the parameter space that most increase the probability of taking that action $A_t$ when we account state $S_t$
		- multiply by $G_t$ : move the parameter vector and amount proportional to the return $G_t$ -> action $A_t$ that yield higher return cause $\theta_t$ to move more
		- divide by $\pi(A_t...)$ : ensure that the action that yield higher result but have low estimated frequency be updated more robustly
	- -> Monte Carlo algorithm
- algorithm ![[Pasted image 20240103231511.png]]
- performance on short-corridor gridworld ![[Pasted image 20240103231618.png]]
## 13.4   REINFORCE with Baseline
- $$\nabla J(\theta) = \sum_s\mu(s)\sum_a(q_{\pi}(s,a) - b(s))\nabla\pi(a|s,\theta)$$
	- the baseline can be any function that doesn't vary with $a$
	- the equatioin remains valid because the subtracted quantity is zero $$\begin{align}\sum_ab(s)\nabla \pi (a | s, \theta) & = b(s)\nabla \sum_a\pi(a | s, \theta) \\ 
		& = b(s)\nabla 1 \\
		& = 0\end{align}$$
- new update formula $$\theta_{t+1} = \theta_t + \alpha (G_t - b(S_t)) \frac{\nabla \pi(A_t| S_t,\theta_t)}{\pi(A_t | S_t, \theta_t)} $$
- rule to choose $b(s)$ : high baseline should be applied to states where all actions have high value to differentiate between higher value action from less highly valued one, and vice versa
- one natural choice is an estimate of the state value $\hat v(S_t,w)$ -> state value can be calculate by any learned methods, but since REINFORCE is a Monte Carlo algo, we'll also use Monte Carlo method to calculate $w$
- algorithm ![[Pasted image 20240103232413.png]]
- performance comparison ![[Pasted image 20240103232510.png]]
## 13.5 (Advantage) Actor - Critic Methods
- Actor - Critic Methods is (indeed) obtained from the alternation of $G_t$ in REINFORCE by $Q(S_t,A_t)$ $$\theta_{t+1} = \theta_t + \hat q(S_t, A_t)\frac{\nabla \pi (A_t | S_t,\theta_t)}{\pi(A_t|S_t, \theta_t)}$$
- update formula for Advantage Actor - Critic (A2C) method (derived from REINFORCE with baseline) $$\begin{align}
\theta_{t+1} & = \theta_t + (G_{t:t+1} - \hat v(S_t, w_t))\frac{\nabla \pi (A_t | S_t,\theta_t)}{\pi(A_t|S_t, \theta_t)} \\
& = \theta_t + (R_{t+1} + \gamma \hat v(S_{t+1}, w_t) - \hat v(S_t,w_t))\frac{\nabla \pi (A_t | S_t,\theta_t)}{\pi(A_t|S_t, \theta_t)} \\
\end{align}$$
- algorithm ![[Pasted image 20240104125009.png]]
## 13.6 Policy Gradient with Continuous Problems
- algorithm ![[Pasted image 20240104174521.png]]
## 13.7 Policy Parameterization for Continuous Actions
- $$\pi(a | s, \theta) = \frac{1}{\sigma(s,\theta)\sqrt{2\pi}}\exp\left[\frac{(a - \mu(s,\theta))^2}{2\sigma^2(s,\theta)}\right]$$
- we divide the parameter vector into 2 part: 1 for approximation of the mean, 1 for approximation of the standard deviation
	- the mean can be approximated as a linear function $$\mu(s, \theta) = \theta_\mu^\top x_\mu(s)$$
	- the standard deviation is better approximated as an exponential of a linear function $$\sigma(s, \theta) = \exp(\theta_\sigma^\top x_\sigma(s))$$