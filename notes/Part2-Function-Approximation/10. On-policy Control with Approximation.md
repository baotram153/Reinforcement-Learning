## 1. Episodic Semi-Gradient Control
- $$w_{t+1} = w_t + \alpha[q_\pi(S_t, A_t) - \hat q(S_t, A_t, w_t))]\nabla \hat q(S_t,A_t, w_t)$$
- example for 1-step sarsa $$w_{t+1} = w_{t} + \alpha[R_{t+1} + \gamma \hat q(S_{t+1}, A_{t+1}, w_t)-\hat q(S_t,A_t,w_t)]\nabla \hat q(S_t, A_t, w_T)$$
- control problem
	- how to choose action for each time step? if action space is small, actions are chosen to maximize $A_{t+1}^* = \arg\max_a \hat q (S_t, a, w_t)$ ($\epsilon-$ greedy)
	- how the policy is improved? by chosen action to maximized action-value func -> policy becomes optimal policy
- algo ![[Pasted image 20231121115153.png]]
- example: Mountain car
	- example of a control task where things have to get worse in the beginning in order to get better
	- 3 actions: throttle forward (+1), throttle reverse (-1), zero throttle (0)
	- 2 dimensions of state: position ($x_t$) and velocity ($\dot x_t$)
		- $x_{t+1}(x_t, \dot x_{t+1})$
		- $\dot x_{t+1}(A_t, x_t, \dot x_t)$
	- $-1.2 \leq x_{t+1} \leq 0.5$ and $-0.7 \leq \dot x_{t+1} \leq 0.7$ -> convert to binary feature vector using grid tiling -> 8 tilings, each covers 1/8 of the bounded distance in each dimension
	- ![[Pasted image 20231127171653.png]]
	- ![[Pasted image 20231125014433.png]]
## 2. Semi-Grandient n-step SARSA
- $$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \gamma^n \hat q(S_{t+n},A_{t+n} w_{t+n-1})$$
- $$w_{t+n} = w_{t+n-1} + (G_{t:t+n} - \hat q(S_{t+n}, A_{t+n}, w_{t+n-1}))\nabla \hat q(S_{t+n}, A_{t+n}, w_{t+n-1})$$
- semi-gradient n-step sarsa algo ![[Pasted image 20231121133129.png]]
- performance between difference step size ![[Pasted image 20231121133308.png]]
## 3. Average Reward
- evaluate a policy? discounted reward -> average reward
- when $t \rightarrow \infty$, $r(\pi)$ only depends on policy $\pi$ (distribution of actions taken from specific state) and MDP transition probability (distribution of states and rewards of specific state + action)
- (?)$$\begin{align} 
r_t(\pi) & = \lim_{h \rightarrow \infty}\frac{1}{h}\sum_{t=1}^h \mathbb E[R_t |S_0, A_{0:t-1} \sim \pi] \\ 
& = \lim_{t\rightarrow \infty}\mathbb E [R_t|S_0, A_{0:t-1} \sim \pi] \\
& = \sum_s \mu(s) \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)r \end{align}$$
- second and third equation only hold if the steady-state distribution $\mu(s) = \lim_{t \rightarrow \infty} Pr\{S_t = s | \cancel S_0, A_{0:t-1} \sim \pi\}$ exists and independent of $S_0$ -> ergodic
- $r(\pi)$ : average reward or reward rate. Every policies that have the highest $r(\pi)$ are considered optimal policy
- property of steady state distribution $$\sum_s\mu_\pi(s)\sum_a\pi(a|s)p(s',r|s,a) = \mu_\pi(s')$$
- in average reward setting, returns are defined in terms of differences between rewards and the average reward (differential return)
	- $G_t = R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + ... + R_{t+3} - r(\pi)$
- the corresponding value function is called differential value function
	- $$v_\pi(s) = \sum_a\pi(a|s)\sum_{s'}p(r,s'|a,s)(r-r(\pi) + \hat v(s'))$$
	- $$q_\pi(s,a) = \sum_a\pi(a|s)\sum_{s'}p(r,s'|a,s)(r-r(\pi) + \sum_{a'} \pi(a'|s')\hat q(s',a'))$$
	- corresponding Bellman equation: 
		- $$v_*(s) = \max_a\sum_{s'}p(r,s'|a,s)(r-\max_\pi r(\pi) + \hat v_*(s'))$$
		- $$q_*(s,a) = \sum_a\pi(a|s)\sum_{s'}p(r,s'|a,s)(r-\max_\pi r(\pi) + \max_{a'}\hat q_*(s',a'))$$
- differential form of the TD errors
	- $\delta_t = R_{t+1} - \overline R + \hat v(S_{t+1}, w_t) - \hat v(S_t, w_t)$
	-  $\delta_t = R_{t+1} - \overline R + \hat q(S_{t+1}, A_{t+1}, w_t) - \hat q(S_t, A_t w_t)$
	- corresponding parameter update function
		- $w_{t+1} = w_t + \alpha \delta_t \nabla \hat q(S_{t+1}, A_{t+1}, w_t)$
- one disadvantage is that the algorithm doesn't converges to differential value but differential value plus some offset
	- how this algorithm could be changed to eliminate the offset? (topic for future research)
- differential semi-gradient Sarsa ![[Pasted image 20231121170304.png]]
- exercise 10.6 (excercise này em nghĩ nó quan trọng, nhưng em đọc chưa hiểu hì hì)
- example 10.2: Access-control queuing task
	- state dimensions: priority & number of servers available
	- ![[Pasted image 20231121172112.png]]
	- training steps: 2 million
	- the drop on the right is due to unsufficient data
## 4. Deprecating discount setting
- this part explains why discounting is unnecessary in on-policy control with function approximation
## 5. Differential Semi-gradient n-step SARSA
- $$G_{t:t+n} = R_{t+1} - \overline R + R_{t+2} - \overline R + ... + R_{t+n}-\overline R + \hat q(S_{t+n}, A_{t+n}, w_{t+n-1})$$
- $$\delta_t = G_{t:t+n} - \hat q(S_t, A_t, w_{t+n-1})$$
- algo ![[Pasted image 20231121182617.png]]